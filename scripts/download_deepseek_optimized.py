#!/usr/bin/env python3
"""
å„ªåŒ–çš„ DeepSeek æ¨¡å‹ä¸‹è¼‰è…³æœ¬
æ”¯æ´æ–·é»çºŒå‚³ã€é€²åº¦é¡¯ç¤ºå’ŒéŒ¯èª¤æ¢å¾©
"""

import os
import sys
import time
from pathlib import Path
from huggingface_hub import snapshot_download, hf_hub_download
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import subprocess

def check_available_models():
    """æª¢æŸ¥å¯ç”¨çš„ DeepSeek æ¨¡å‹é¸é …"""
    models = {
        "1": {
            "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
            "size": "~3.5GB",
            "description": "åŸè¨ˆåŠƒæ¨¡å‹ - è¼ƒå¤§ä½†åŠŸèƒ½å®Œæ•´"
        },
        "2": {
            "name": "deepseek-ai/deepseek-coder-1.3b-base",
            "size": "~2.6GB", 
            "description": "ç¨‹å¼ç¢¼å°ˆç”¨æ¨¡å‹ - é©åˆæŠ€è¡“åˆ†æ"
        },
        "3": {
            "name": "microsoft/DialoGPT-medium",
            "size": "~1.4GB",
            "description": "å°è©±æ¨¡å‹ - è¼ƒå°ä½†é©åˆæ¸¬è©¦"
        }
    }
    
    print("ğŸ¤– å¯ç”¨çš„æ¨¡å‹é¸é …:")
    for key, model in models.items():
        print(f"   {key}. {model['name']}")
        print(f"      å¤§å°: {model['size']}")
        print(f"      èªªæ˜: {model['description']}\n")
    
    return models

def download_with_cli(model_name, target_dir):
    """ä½¿ç”¨ huggingface-cli ä¸‹è¼‰ (æ”¯æ´æ–·é»çºŒå‚³)"""
    print(f"ğŸš€ ä½¿ç”¨ CLI ä¸‹è¼‰æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ç›®æ¨™ç›®éŒ„: {target_dir}")
    
    cmd = [
        sys.executable, "-m", "huggingface_hub.commands.huggingface_cli",
        "download", model_name,
        "--local-dir", str(target_dir),
        "--resume-download"
    ]
    
    print("ğŸ“¥ é–‹å§‹ä¸‹è¼‰ (æ”¯æ´æ–·é»çºŒå‚³)...")
    print("ğŸ’¡ æç¤º: å¦‚æœä¸‹è¼‰ä¸­æ–·ï¼Œé‡æ–°åŸ·è¡Œæ­¤è…³æœ¬å³å¯å¾æ–·é»ç¹¼çºŒ")
    
    try:
        result = subprocess.run(cmd, check=True, text=True, capture_output=False)
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ CLI ä¸‹è¼‰å¤±æ•—: {e}")
        return False
    except KeyboardInterrupt:
        print("\nâ¸ï¸ ä¸‹è¼‰è¢«ç”¨æˆ¶ä¸­æ–·")
        print("ğŸ’¡ é‡æ–°åŸ·è¡Œæ­¤è…³æœ¬å¯ä»¥å¾ä¸­æ–·é»ç¹¼çºŒä¸‹è¼‰")
        return False

def download_with_python(model_name, target_dir):
    """ä½¿ç”¨ Python API ä¸‹è¼‰"""
    print(f"ğŸ ä½¿ç”¨ Python API ä¸‹è¼‰: {model_name}")
    
    try:
        print("ğŸ“¥ ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ...")
        snapshot_download(
            repo_id=model_name,
            local_dir=target_dir,
            resume_download=True,
            local_dir_use_symlinks=False
        )
        return True
    except Exception as e:
        print(f"âŒ Python API ä¸‹è¼‰å¤±æ•—: {e}")
        return False

def test_model(model_path, tokenizer_path=None):
    """æ¸¬è©¦ä¸‹è¼‰çš„æ¨¡å‹"""
    print("\nğŸ§ª æ¸¬è©¦æ¨¡å‹...")
    
    try:
        if tokenizer_path is None:
            tokenizer_path = model_path
            
        print("   è¼‰å…¥ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        
        print("   è¼‰å…¥æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto" if not sys.platform.startswith('darwin') else "cpu"
        )
        
        print("   åŸ·è¡Œæ¨ç†æ¸¬è©¦...")
        test_input = "Qubic æ˜¯ä»€éº¼ï¼Ÿ"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… æ¸¬è©¦æˆåŠŸ!")
        print(f"   è¼¸å…¥: {test_input}")
        print(f"   è¼¸å‡º: {response[:150]}...")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
        print("   æ¨¡å‹å·²ä¸‹è¼‰ä½†å¯èƒ½éœ€è¦èª¿æ•´é…ç½®")
        return False

def save_model_config(model_name, model_path, success=True):
    """ä¿å­˜æ¨¡å‹é…ç½®"""
    config = {
        "model_name": model_name,
        "model_path": str(model_path.absolute()),
        "download_date": time.strftime("%Y-%m-%d %H:%M:%S"),
        "download_success": success,
        "status": "ready" if success else "partial",
        "notes": "Model ready for inference" if success else "Download incomplete or test failed"
    }
    
    config_path = model_path.parent / "deepseek_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {config_path}")
    return config_path

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ DeepSeek æ¨¡å‹ä¸‹è¼‰å™¨")
    print("=" * 50)
    
    # æª¢æŸ¥å¯ç”¨æ¨¡å‹
    models = check_available_models()
    
    # ç”¨æˆ¶é¸æ“‡
    choice = input("è«‹é¸æ“‡è¦ä¸‹è¼‰çš„æ¨¡å‹ (1-3, é è¨­=2): ").strip() or "2"
    
    if choice not in models:
        print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨é è¨­æ¨¡å‹")
        choice = "2"
    
    selected_model = models[choice]
    model_name = selected_model["name"]
    
    print(f"\nâœ… å·²é¸æ“‡: {model_name}")
    print(f"ğŸ“Š é ä¼°å¤§å°: {selected_model['size']}")
    
    # è¨­ç½®ä¸‹è¼‰è·¯å¾‘
    models_dir = Path("backend/ai/models")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = models_dir / "deepseek"
    model_path.mkdir(exist_ok=True)
    
    print(f"ğŸ“ ä¸‹è¼‰ç›®éŒ„: {model_path.absolute()}")
    
    # ç¢ºèªä¸‹è¼‰
    confirm = input("\nç¢ºå®šè¦é–‹å§‹ä¸‹è¼‰å—? (y/N): ").strip().lower()
    if confirm != 'y':
        print("âŒ ä¸‹è¼‰å·²å–æ¶ˆ")
        return False
    
    # å˜—è©¦ä¸‹è¼‰
    print(f"\nğŸ”„ é–‹å§‹ä¸‹è¼‰ {model_name}...")
    start_time = time.time()
    
    # å„ªå…ˆä½¿ç”¨ CLI æ–¹å¼ (æ›´ç©©å®š)
    success = download_with_cli(model_name, model_path)
    
    if not success:
        print("\nğŸ”„ CLI ä¸‹è¼‰å¤±æ•—ï¼Œå˜—è©¦ Python API...")
        success = download_with_python(model_name, model_path)
    
    download_time = time.time() - start_time
    
    if success:
        print(f"\nğŸ‰ æ¨¡å‹ä¸‹è¼‰å®Œæˆ! (è€—æ™‚: {download_time:.1f}ç§’)")
        
        # æ¸¬è©¦æ¨¡å‹ (å¯é¸)
        test_choice = input("æ˜¯å¦è¦æ¸¬è©¦æ¨¡å‹? (y/N): ").strip().lower()
        if test_choice == 'y':
            try:
                import torch
                test_success = test_model(model_path)
            except ImportError:
                print("âš ï¸ PyTorch æœªå®‰è£ï¼Œè·³éæ¨¡å‹æ¸¬è©¦")
                test_success = True
        else:
            test_success = True
            
        # ä¿å­˜é…ç½®
        config_path = save_model_config(model_name, model_path, test_success)
        
        print(f"\nâœ… è¨­ç½®å®Œæˆ!")
        print(f"ğŸ“‹ ä¸‹ä¸€æ­¥:")
        print(f"   1. å»ºç«‹æ¨ç† API: python scripts/setup_inference_api.py")
        print(f"   2. æ•´åˆåˆ° QDashboard")
        print(f"   3. é–‹å§‹ AI åˆ†æåŠŸèƒ½é–‹ç™¼")
        
        return True
    else:
        print(f"\nâŒ ä¸‹è¼‰å¤±æ•—")
        print("ğŸ”§ å»ºè­°:")
        print("   1. æª¢æŸ¥ç¶²è·¯é€£æ¥")
        print("   2. ç¢ºèªç£ç¢Ÿç©ºé–“å……è¶³")
        print("   3. é‡æ–°åŸ·è¡Œæ­¤è…³æœ¬ (æ”¯æ´æ–·é»çºŒå‚³)")
        
        # ä»ç„¶ä¿å­˜é…ç½®ä»¥è¨˜éŒ„ç‹€æ…‹
        save_model_config(model_name, model_path, False)
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸ ç”¨æˆ¶ä¸­æ–·ä¸‹è¼‰")
        print("ğŸ’¡ é‡æ–°åŸ·è¡Œæ­¤è…³æœ¬å¯ä»¥å¾ä¸­æ–·é»ç¹¼çºŒ")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ æœªé æœŸçš„éŒ¯èª¤: {e}")
        sys.exit(1)

