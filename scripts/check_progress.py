#!/usr/bin/env python3
"""
å°ˆæ¡ˆé€²åº¦æª¢æŸ¥è…³æœ¬
æª¢æŸ¥æ‰€æœ‰çµ„ä»¶ç‹€æ…‹ä¸¦æ›´æ–°é…ç½®
"""

import os
import sys
import json
import time
from pathlib import Path

def check_environment():
    """æª¢æŸ¥ç’°å¢ƒè¨­ç½®"""
    print("ğŸ” æª¢æŸ¥ç’°å¢ƒè¨­ç½®...")
    
    checks = {
        "Python ç‰ˆæœ¬": sys.version_info >= (3, 9),
        "è™›æ“¬ç’°å¢ƒ": os.environ.get('VIRTUAL_ENV') is not None,
        "å·¥ä½œç›®éŒ„": Path.cwd().name == "deepseek-qubic-ai"
    }
    
    for check, result in checks.items():
        status = "âœ…" if result else "âŒ"
        print(f"   {status} {check}: {result}")
    
    return all(checks.values())

def check_dependencies():
    """æª¢æŸ¥ä¾è³´å¥—ä»¶"""
    print("\nğŸ“¦ æª¢æŸ¥ä¾è³´å¥—ä»¶...")
    
    required_packages = [
        "torch", "transformers", "accelerate", 
        "bitsandbytes", "huggingface_hub", "flask", "qubipy"
    ]
    
    installed = {}
    for package in required_packages:
        try:
            __import__(package)
            if package == "torch":
                import torch
                version = torch.__version__
            elif package == "transformers":
                import transformers
                version = transformers.__version__
            else:
                version = "å·²å®‰è£"
            installed[package] = version
            print(f"   âœ… {package}: {version}")
        except ImportError:
            installed[package] = None
            print(f"   âŒ {package}: æœªå®‰è£")
    
    return installed

def check_models():
    """æª¢æŸ¥æ¨¡å‹ç‹€æ…‹"""
    print("\nğŸ¤– æª¢æŸ¥æ¨¡å‹ç‹€æ…‹...")
    
    models_dir = Path("backend/ai/models")
    if not models_dir.exists():
        print("   âŒ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨")
        return {}
    
    # æª¢æŸ¥æ¸¬è©¦æ¨¡å‹
    test_model = models_dir / "test_model"
    test_status = "âœ… å¯ç”¨" if test_model.exists() else "âŒ ç¼ºå¤±"
    print(f"   ğŸ“ æ¸¬è©¦æ¨¡å‹ (GPT2): {test_status}")
    
    # æª¢æŸ¥ DeepSeek æ¨¡å‹
    deepseek_dir = models_dir / "deepseek"
    deepseek_model = deepseek_dir / "model.safetensors"
    
    if deepseek_model.exists():
        size_gb = deepseek_model.stat().st_size / (1024**3)
        print(f"   ğŸ¯ DeepSeek æ¨¡å‹: âœ… å·²ä¸‹è¼‰ ({size_gb:.2f}GB)")
        
        # æª¢æŸ¥é…ç½®æª”æ¡ˆ
        config_file = models_dir / "deepseek_config.json"
        if config_file.exists():
            with open(config_file, 'r') as f:
                config = json.load(f)
            print(f"   ğŸ“‹ é…ç½®ç‹€æ…‹: {config.get('status', 'æœªçŸ¥')}")
            
            # å˜—è©¦æ¸¬è©¦æ¨ç†
            test_result = test_deepseek_inference()
            if test_result:
                # æ›´æ–°é…ç½®ç‚ºæˆåŠŸ
                config['download_success'] = True
                config['status'] = 'ready'
                config['notes'] = 'Model ready for inference'
                config['last_test'] = time.strftime("%Y-%m-%d %H:%M:%S")
                
                with open(config_file, 'w') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
                print("   âœ… æ¨ç†æ¸¬è©¦: æˆåŠŸ")
            else:
                print("   âš ï¸ æ¨ç†æ¸¬è©¦: éœ€è¦èª¿æ•´")
        else:
            print("   âš ï¸ é…ç½®æª”æ¡ˆ: ç¼ºå¤±")
    else:
        print("   âŒ DeepSeek æ¨¡å‹: æœªä¸‹è¼‰")
    
    return {
        "test_model": test_model.exists(),
        "deepseek_model": deepseek_model.exists(),
        "model_size_gb": size_gb if deepseek_model.exists() else 0
    }

def test_deepseek_inference():
    """æ¸¬è©¦ DeepSeek æ¨ç†åŠŸèƒ½"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_path = "backend/ai/models/deepseek"
        
        # è¼‰å…¥ tokenizer å’Œæ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,  # ä½¿ç”¨ float32 ç¢ºä¿ç›¸å®¹æ€§
            device_map="cpu"  # å¼·åˆ¶ä½¿ç”¨ CPU
        )
        
        # ç°¡å–®æ¸¬è©¦
        test_input = "Hello, how are you?"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return True
        
    except Exception as e:
        print(f"   âš ï¸ æ¨ç†æ¸¬è©¦éŒ¯èª¤: {e}")
        return False

def generate_progress_report():
    """ç”Ÿæˆé€²åº¦å ±å‘Š"""
    print("\n" + "="*50)
    print("ğŸ“Š å°ˆæ¡ˆé€²åº¦ç¸½çµ")
    print("="*50)
    
    # æª¢æŸ¥å„é …ç›®
    env_ok = check_environment()
    deps = check_dependencies()
    models = check_models()
    
    # è¨ˆç®—å®Œæˆåº¦
    total_tasks = 10
    completed_tasks = 0
    
    if env_ok:
        completed_tasks += 3
    if all(deps.values()):
        completed_tasks += 3
    if models.get("test_model"):
        completed_tasks += 1
    if models.get("deepseek_model"):
        completed_tasks += 2
    if models.get("model_size_gb", 0) > 3:  # æ¨¡å‹å®Œæ•´ä¸‹è¼‰
        completed_tasks += 1
    
    completion_rate = (completed_tasks / total_tasks) * 100
    
    print(f"\nğŸ¯ æ•´é«”å®Œæˆåº¦: {completion_rate:.1f}% ({completed_tasks}/{total_tasks})")
    
    # ç‹€æ…‹åˆ†é¡
    if completion_rate >= 90:
        print("âœ… ç‹€æ…‹: æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹ API é–‹ç™¼")
    elif completion_rate >= 70:
        print("ğŸ”„ ç‹€æ…‹: åŸºæœ¬å®Œæˆï¼Œéœ€è¦å¾®èª¿")
    elif completion_rate >= 50:
        print("âš ï¸ ç‹€æ…‹: é€²è¡Œä¸­ï¼Œéœ€è¦ç¹¼çºŒå®Œæˆ")
    else:
        print("âŒ ç‹€æ…‹: éœ€è¦é‡æ–°è¨­ç½®")
    
    # ä¸‹ä¸€æ­¥å»ºè­°
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè­°:")
    if completion_rate >= 80:
        print("   1. å»ºç«‹æ¨ç† API ç«¯é»")
        print("   2. æ•´åˆåˆ° QDashboard")
        print("   3. é–‹ç™¼ AI åˆ†æåŠŸèƒ½")
    else:
        print("   1. å®Œæˆæ¨¡å‹è¨­ç½®å’Œæ¸¬è©¦")
        print("   2. è§£æ±ºä»»ä½•å‰©é¤˜çš„ä¾è³´å•é¡Œ")
        print("   3. é©—è­‰æ¨ç†åŠŸèƒ½")
    
    return {
        "completion_rate": completion_rate,
        "environment": env_ok,
        "dependencies": deps,
        "models": models
    }

if __name__ == "__main__":
    report = generate_progress_report()
    
    # ä¿å­˜å ±å‘Š
    report_file = Path("progress_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ é€²åº¦å ±å‘Šå·²ä¿å­˜: {report_file}")

