#!/usr/bin/env python3
"""
æ·±åº¦é©—è­‰ DeepSeek æ¨¡å‹æ¨ç†åŠŸèƒ½
é€²è¡Œå¤šç¨®æ¸¬è©¦ä»¥ç¢ºä¿æ¨¡å‹æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import traceback

def test_model_loading():
    """æ¸¬è©¦æ¨¡å‹è¼‰å…¥"""
    print("ğŸ”„ æ¸¬è©¦ 1: æ¨¡å‹è¼‰å…¥...")
    
    model_path = "backend/ai/models/deepseek"
    
    try:
        # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆå­˜åœ¨
        model_file = Path(model_path) / "model.safetensors"
        if not model_file.exists():
            print(f"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {model_file}")
            return False
        
        print(f"   âœ… æ¨¡å‹æª”æ¡ˆå­˜åœ¨: {model_file.stat().st_size / (1024**3):.2f}GB")
        
        # è¼‰å…¥ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        print(f"   âœ… Tokenizer è¼‰å…¥æˆåŠŸ: {tokenizer.__class__.__name__}")
        
        # è¼‰å…¥æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        print(f"   âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model.__class__.__name__}")
        
        return True, tokenizer, model
        
    except Exception as e:
        print(f"   âŒ è¼‰å…¥å¤±æ•—: {e}")
        traceback.print_exc()
        return False, None, None

def test_tokenization(tokenizer):
    """æ¸¬è©¦ tokenization"""
    print("\nğŸ”„ æ¸¬è©¦ 2: Tokenization...")
    
    try:
        test_texts = [
            "Hello, how are you?",
            "Qubic æ˜¯ä»€éº¼ï¼Ÿ",
            "è§£é‡‹ä¸€ä¸‹å€å¡ŠéˆæŠ€è¡“ã€‚",
            "What is artificial intelligence?"
        ]
        
        for text in test_texts:
            inputs = tokenizer(text, return_tensors="pt")
            tokens = tokenizer.tokenize(text)
            print(f"   âœ… '{text}' â†’ {len(tokens)} tokens")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Tokenization å¤±æ•—: {e}")
        return False

def test_inference(tokenizer, model):
    """æ¸¬è©¦æ¨ç†åŠŸèƒ½"""
    print("\nğŸ”„ æ¸¬è©¦ 3: æ¨ç†åŠŸèƒ½...")
    
    try:
        test_cases = [
            {
                "input": "Hello, how are you?",
                "max_length": 50,
                "description": "åŸºæœ¬è‹±æ–‡å°è©±"
            },
            {
                "input": "Qubic æ˜¯ä»€éº¼ï¼Ÿ",
                "max_length": 100,
                "description": "ä¸­æ–‡æŠ€è¡“å•é¡Œ"
            },
            {
                "input": "Explain blockchain technology.",
                "max_length": 80,
                "description": "æŠ€è¡“è§£é‡‹"
            }
        ]
        
        inference_results = []
        
        for i, case in enumerate(test_cases, 1):
            print(f"\n   æ¸¬è©¦ 3.{i}: {case['description']}")
            print(f"   è¼¸å…¥: {case['input']}")
            
            start_time = time.time()
            
            # Tokenize è¼¸å…¥
            inputs = tokenizer(case['input'], return_tensors="pt")
            
            # ç”Ÿæˆå›æ‡‰
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=case['max_length'],
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç¢¼çµæœ
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            inference_time = time.time() - start_time
            
            print(f"   æ¨ç†æ™‚é–“: {inference_time:.2f}ç§’")
            print(f"   è¼¸å‡ºé•·åº¦: {len(response)} å­—ç¬¦")
            print(f"   è¼¸å‡ºå…§å®¹: {response[:150]}...")
            
            # æª¢æŸ¥è¼¸å‡ºå“è³ª
            if len(response) > len(case['input']) and response.startswith(case['input']):
                print(f"   âœ… æ¨ç†æˆåŠŸ")
                inference_results.append(True)
            else:
                print(f"   âš ï¸ æ¨ç†çµæœå¯èƒ½æœ‰å•é¡Œ")
                inference_results.append(False)
        
        success_rate = sum(inference_results) / len(inference_results) * 100
        print(f"\n   ğŸ“Š æ¨ç†æˆåŠŸç‡: {success_rate:.1f}% ({sum(inference_results)}/{len(inference_results)})")
        
        return success_rate >= 66.7  # è‡³å°‘ 2/3 æˆåŠŸ
        
    except Exception as e:
        print(f"   âŒ æ¨ç†æ¸¬è©¦å¤±æ•—: {e}")
        traceback.print_exc()
        return False

def test_model_configuration():
    """æ¸¬è©¦æ¨¡å‹é…ç½®"""
    print("\nğŸ”„ æ¸¬è©¦ 4: æ¨¡å‹é…ç½®...")
    
    try:
        model_path = Path("backend/ai/models/deepseek")
        
        # æª¢æŸ¥é…ç½®æª”æ¡ˆ
        config_files = ["config.json", "tokenizer_config.json", "generation_config.json"]
        
        for config_file in config_files:
            file_path = model_path / config_file
            if file_path.exists():
                print(f"   âœ… {config_file}: å­˜åœ¨")
            else:
                print(f"   âŒ {config_file}: ç¼ºå¤±")
                return False
        
        return True
        
    except Exception as e:
        print(f"   âŒ é…ç½®æª¢æŸ¥å¤±æ•—: {e}")
        return False

def test_memory_usage(model):
    """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨"""
    print("\nğŸ”„ æ¸¬è©¦ 5: è¨˜æ†¶é«”ä½¿ç”¨...")
    
    try:
        # ç²å–æ¨¡å‹åƒæ•¸æ•¸é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"   ğŸ“Š ç¸½åƒæ•¸æ•¸é‡: {total_params:,}")
        print(f"   ğŸ“Š å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
        
        # ä¼°ç®—è¨˜æ†¶é«”ä½¿ç”¨ (Float32 = 4 bytes per parameter)
        memory_mb = (total_params * 4) / (1024 * 1024)
        print(f"   ğŸ“Š ä¼°ç®—è¨˜æ†¶é«”ä½¿ç”¨: {memory_mb:.1f}MB")
        
        if memory_mb < 10000:  # å°æ–¼ 10GB æ˜¯åˆç†çš„
            print(f"   âœ… è¨˜æ†¶é«”ä½¿ç”¨åˆç†")
            return True
        else:
            print(f"   âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨éé«˜")
            return False
        
    except Exception as e:
        print(f"   âŒ è¨˜æ†¶é«”æª¢æŸ¥å¤±æ•—: {e}")
        return False

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ DeepSeek æ¨¡å‹æ·±åº¦é©—è­‰")
    print("=" * 50)
    
    # æª¢æŸ¥ç’°å¢ƒ
    print(f"ğŸ” ç’°å¢ƒè³‡è¨Š:")
    print(f"   Python: {sys.version}")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   å·¥ä½œç›®éŒ„: {Path.cwd()}")
    print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    test_results = []
    
    # æ¸¬è©¦ 1: æ¨¡å‹è¼‰å…¥
    load_success, tokenizer, model = test_model_loading()
    test_results.append(("æ¨¡å‹è¼‰å…¥", load_success))
    
    if not load_success:
        print("\nâŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒå¾ŒçºŒæ¸¬è©¦")
        return False
    
    # æ¸¬è©¦ 2: Tokenization
    token_success = test_tokenization(tokenizer)
    test_results.append(("Tokenization", token_success))
    
    # æ¸¬è©¦ 3: æ¨ç†åŠŸèƒ½
    inference_success = test_inference(tokenizer, model)
    test_results.append(("æ¨ç†åŠŸèƒ½", inference_success))
    
    # æ¸¬è©¦ 4: æ¨¡å‹é…ç½®
    config_success = test_model_configuration()
    test_results.append(("æ¨¡å‹é…ç½®", config_success))
    
    # æ¸¬è©¦ 5: è¨˜æ†¶é«”ä½¿ç”¨
    memory_success = test_memory_usage(model)
    test_results.append(("è¨˜æ†¶é«”ä½¿ç”¨", memory_success))
    
    # ç¸½çµ
    print("\n" + "=" * 50)
    print("ğŸ“Š é©—è­‰çµæœç¸½çµ")
    print("=" * 50)
    
    for test_name, result in test_results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"   {test_name}: {status}")
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    success_rate = (passed_tests / total_tests) * 100
    
    print(f"\nğŸ¯ ç¸½é«”æ¸¬è©¦çµæœ: {success_rate:.1f}% ({passed_tests}/{total_tests})")
    
    if success_rate >= 80:
        print("âœ… DeepSeek æ¨¡å‹æ¨ç†åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥ä½¿ç”¨")
        return True
    elif success_rate >= 60:
        print("âš ï¸ DeepSeek æ¨¡å‹åŸºæœ¬å¯ç”¨ï¼Œä½†å¯èƒ½éœ€è¦èª¿æ•´")
        return True
    else:
        print("âŒ DeepSeek æ¨¡å‹æœ‰é‡å¤§å•é¡Œï¼Œéœ€è¦é‡æ–°è¨­ç½®")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâ¸ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc()
        sys.exit(1)

