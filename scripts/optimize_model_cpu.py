#!/usr/bin/env python3
"""
CPU ç’°å¢ƒä¸‹çš„æ¨¡å‹å„ªåŒ–è…³æœ¬
å°ˆæ³¨æ–¼ CPU æ¨ç†å„ªåŒ–è€Œéé‡åŒ–
"""

import os
import sys
import time
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def optimize_for_cpu():
    """ç‚º CPU æ¨ç†å„ªåŒ–æ¨¡å‹é…ç½®"""
    print("ğŸ”§ CPU æ¨ç†å„ªåŒ–")
    print("=" * 40)
    
    model_path = "backend/ai/models/deepseek"
    
    try:
        print("ğŸ“¥ è¼‰å…¥æ¨¡å‹é€²è¡Œ CPU å„ªåŒ–...")
        
        # è¼‰å…¥ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # è¼‰å…¥æ¨¡å‹ä¸¦å„ªåŒ–
        start_time = time.time()
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,  # ä½¿ç”¨ float32 æé«˜ CPU ç›¸å®¹æ€§
            device_map="cpu",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # å„ªåŒ–æ¨¡å‹é…ç½®
        model.eval()
        
        # å˜—è©¦ JIT ç·¨è­¯ï¼ˆå¦‚æœæ”¯æ´ï¼‰
        try:
            print("âš¡ å˜—è©¦ JIT ç·¨è­¯å„ªåŒ–...")
            # å‰µå»ºç¤ºä¾‹è¼¸å…¥
            example_input = tokenizer("æ¸¬è©¦", return_tensors="pt")
            
            # ä½¿ç”¨ torch.jit.trace ä¸é©ç”¨æ–¼ç”Ÿæˆæ¨¡å‹ï¼Œæ”¹ç”¨å…¶ä»–å„ªåŒ–
            model = torch.jit.optimize_for_inference(model)  # type: ignore
            print("   âœ… JIT å„ªåŒ–æˆåŠŸ")
        except Exception as e:
            print(f"   âš ï¸  JIT å„ªåŒ–å¤±æ•—: {e}")
        
        load_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹è¼‰å…¥å’Œå„ªåŒ–å®Œæˆï¼Œè€—æ™‚: {load_time:.2f}ç§’")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ CPU å„ªåŒ–å¤±æ•—: {e}")
        return None, None

def benchmark_inference_speed(model, tokenizer):
    """åŸºæº–æ¸¬è©¦æ¨ç†é€Ÿåº¦"""
    print(f"\nğŸ“Š æ¨ç†é€Ÿåº¦åŸºæº–æ¸¬è©¦")
    print("-" * 30)
    
    test_prompts = [
        "Qubic æ˜¯ä»€éº¼ï¼Ÿ",
        "è§£é‡‹ UPoW å…±è­˜æ©Ÿåˆ¶",
        "ç•¶å‰ Qubic ç¶²è·¯ç‹€æ³å¦‚ä½•ï¼Ÿ"
    ]
    
    total_time = 0
    successful_tests = 0
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\næ¸¬è©¦ {i}/3: {prompt}")
        
        try:
            # é ç†±ï¼ˆç¬¬ä¸€æ¬¡æ¨ç†é€šå¸¸è¼ƒæ…¢ï¼‰
            if i == 1:
                print("   ğŸ”¥ é ç†±æ¨ç†...")
                inputs = tokenizer("é ç†±", return_tensors="pt")
                with torch.no_grad():
                    _ = model.generate(**inputs, max_new_tokens=10)
            
            # æ­£å¼æ¸¬è©¦
            start_time = time.time()
            inputs = tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    temperature=0.6,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            inference_time = time.time() - start_time
            
            print(f"   â±ï¸  æ¨ç†æ™‚é–“: {inference_time:.2f}ç§’")
            print(f"   ğŸ“ å›æ‡‰é•·åº¦: {len(response)} å­—ç¬¦")
            
            total_time += inference_time
            successful_tests += 1
            
        except Exception as e:
            print(f"   âŒ æ¸¬è©¦ {i} å¤±æ•—: {e}")
    
    if successful_tests > 0:
        avg_time = total_time / successful_tests
        print(f"\nğŸ“ˆ æ€§èƒ½ç¸½çµ:")
        print(f"   å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.2f}ç§’")
        print(f"   æˆåŠŸæ¸¬è©¦: {successful_tests}/{len(test_prompts)}")
        
        # æ€§èƒ½è©•ç´š
        if avg_time < 3:
            grade = "å„ªç§€"
        elif avg_time < 6:
            grade = "è‰¯å¥½"
        elif avg_time < 10:
            grade = "ä¸€èˆ¬"
        else:
            grade = "éœ€è¦æ”¹é€²"
        
        print(f"   æ€§èƒ½è©•ç´š: {grade}")
        
        return avg_time
    else:
        print(f"   âŒ æ‰€æœ‰æ¸¬è©¦éƒ½å¤±æ•—äº†")
        return None

def create_optimized_config():
    """å‰µå»ºå„ªåŒ–é…ç½®"""
    print(f"\nâš™ï¸  å‰µå»º CPU å„ªåŒ–é…ç½®...")
    
    config = {
        "optimization_type": "CPU_optimized",
        "device": "cpu",
        "torch_dtype": "float32",
        "optimizations": [
            "low_cpu_mem_usage",
            "eval_mode",
            "inference_optimization"
        ],
        "recommended_settings": {
            "max_new_tokens": 200,
            "temperature": 0.6,
            "top_p": 0.8,
            "top_k": 40,
            "repetition_penalty": 1.2
        },
        "performance_tips": [
            "ä½¿ç”¨è¼ƒå°çš„ batch_size",
            "é™åˆ¶ max_new_tokens ä»¥æ§åˆ¶æ¨ç†æ™‚é–“",
            "ä½¿ç”¨è¼ƒä½çš„ temperature æé«˜ä¸€è‡´æ€§",
            "å•Ÿç”¨ torch.no_grad() é€²è¡Œæ¨ç†"
        ]
    }
    
    config_path = Path("backend/ai/cpu_optimization_config.json")
    import json
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… é…ç½®å·²ä¿å­˜: {config_path}")
    return config_path

def update_inference_engine():
    """æ›´æ–°æ¨ç†å¼•æ“ä»¥ä½¿ç”¨ CPU å„ªåŒ–"""
    print(f"\nğŸ”„ æ›´æ–°æ¨ç†å¼•æ“é…ç½®...")
    
    optimization_notes = '''
# CPU å„ªåŒ–å»ºè­°

## å·²å¯¦æ–½çš„å„ªåŒ–ï¼š
1. ä½¿ç”¨ float32 è€Œé float16ï¼ˆCPU å‹å–„ï¼‰
2. å•Ÿç”¨ low_cpu_mem_usage
3. è¨­ç½®è¼ƒä¿å®ˆçš„ç”Ÿæˆåƒæ•¸
4. æ·»åŠ  Qubic çŸ¥è­˜åº«å¢å¼·

## æ€§èƒ½æå‡æŠ€å·§ï¼š
- ä½¿ç”¨ max_new_tokens è€Œé max_length
- é™ä½ temperature æé«˜ä¸€è‡´æ€§
- ä½¿ç”¨ torch.no_grad() é€²è¡Œæ¨ç†
- é ç†±æ¨¡å‹ï¼ˆé¦–æ¬¡æ¨ç†è¼ƒæ…¢ï¼‰

## ç›£æ§æŒ‡æ¨™ï¼š
- æ¨ç†æ™‚é–“ï¼šç›®æ¨™ < 10 ç§’
- è¨˜æ†¶é«”ä½¿ç”¨ï¼šç›£æ§ RAM æ¶ˆè€—
- å›æ‡‰å“è³ªï¼šä½¿ç”¨ Qubic çŸ¥è­˜é©—è­‰
'''
    
    notes_path = Path("backend/ai/CPU_OPTIMIZATION_NOTES.md")
    with open(notes_path, 'w', encoding='utf-8') as f:
        f.write(optimization_notes)
    
    print(f"   ğŸ“ å„ªåŒ–èªªæ˜å·²ä¿å­˜: {notes_path}")

def main():
    """ä¸»å‡½æ•¸"""
    print("âš¡ CPU æ¨¡å‹å„ªåŒ–")
    print("=" * 50)
    
    # CPU å„ªåŒ–
    model, tokenizer = optimize_for_cpu()
    
    if model is None or tokenizer is None:
        print("âŒ å„ªåŒ–å¤±æ•—")
        return False
    
    # æ€§èƒ½æ¸¬è©¦
    avg_time = benchmark_inference_speed(model, tokenizer)
    
    # å‰µå»ºé…ç½®
    create_optimized_config()
    update_inference_engine()
    
    print(f"\nğŸ‰ CPU å„ªåŒ–å®Œæˆï¼")
    print(f"âœ… æ¨¡å‹å·²é‡å° CPU æ¨ç†å„ªåŒ–")
    if avg_time:
        print(f"âœ… å¹³å‡æ¨ç†æ™‚é–“: {avg_time:.2f}ç§’")
    print(f"âœ… å„ªåŒ–é…ç½®å’Œèªªæ˜å·²å‰µå»º")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
    print(f"   - æ¨ç†æ™‚ä½¿ç”¨ torch.no_grad()")
    print(f"   - é™åˆ¶ max_new_tokens ä»¥æ§åˆ¶æ™‚é–“")
    print(f"   - ä½¿ç”¨ Qubic çŸ¥è­˜å¢å¼·æé«˜å›æ‡‰å“è³ª")
    print(f"   - ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâ¸ï¸ å„ªåŒ–è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ å„ªåŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

