#!/usr/bin/env python3
"""
æ¸¬è©¦æ¨¡å‹è¨­ç½®è…³æœ¬ - ä½¿ç”¨å°å‹æ¨¡å‹é©—è­‰æµç¨‹
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer
from pathlib import Path
import json
import sys
import time

def setup_test_model():
    """è¨­ç½®å°å‹æ¸¬è©¦æ¨¡å‹"""
    print("ğŸ§ª è¨­ç½®æ¸¬è©¦æ¨¡å‹ä»¥é©—è­‰æ¨ç†æµç¨‹...")
    
    # ä½¿ç”¨ GPT2-small (ç´„ 500MB) ä½œç‚ºæ¸¬è©¦
    model_name = "gpt2"
    models_dir = Path("backend/ai/models")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“¦ æ¸¬è©¦æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: {models_dir.absolute()}")
    
    try:
        # ä¸‹è¼‰ tokenizer
        print("\nğŸ“¥ ä¸‹è¼‰ tokenizer...")
        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        tokenizer_path = models_dir / "test_tokenizer"
        tokenizer.save_pretrained(tokenizer_path)
        print(f"âœ… Tokenizer å®Œæˆ: {tokenizer_path}")
        
        # ä¸‹è¼‰æ¨¡å‹
        print("\nğŸ“¥ ä¸‹è¼‰æ¨¡å‹...")
        model = GPT2LMHeadModel.from_pretrained(model_name)
        model_path = models_dir / "test_model"
        model.save_pretrained(model_path)
        print(f"âœ… æ¨¡å‹å®Œæˆ: {model_path}")
        
        # ç°¡å–®æ¸¬è©¦
        print("\nğŸ§ª æ¸¬è©¦æ¨ç†...")
        inputs = tokenizer("Hello, how are you?", return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=50, pad_token_id=tokenizer.eos_token_id)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… æ¸¬è©¦æˆåŠŸ: {response}")
        
        # ä¿å­˜é…ç½®
        config = {
            "model_name": model_name,
            "model_path": str(model_path.absolute()),
            "tokenizer_path": str(tokenizer_path.absolute()),
            "model_type": "test",
            "setup_date": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        config_path = models_dir / "test_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ æ¸¬è©¦æ¨¡å‹è¨­ç½®å®Œæˆ!")
        print(f"ğŸ“‹ é…ç½®: {config_path}")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("   1. æº–å‚™ä¸‹è¼‰æ­£å¼çš„ DeepSeek æ¨¡å‹")
        print("   2. å»ºç«‹æ¨ç† API")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦æ¨¡å‹è¨­ç½®å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    success = setup_test_model()
    sys.exit(0 if success else 1)

