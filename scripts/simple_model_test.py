#!/usr/bin/env python3
"""
ç°¡åŒ–çš„æ¨¡å‹æ¸¬è©¦ - æª¢æŸ¥åŸºæœ¬æ¨ç†åŠŸèƒ½
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

def test_basic_model():
    """æ¸¬è©¦åŸºæœ¬æ¨¡å‹åŠŸèƒ½"""
    
    print("ğŸ” åŸºæœ¬æ¨¡å‹åŠŸèƒ½æ¸¬è©¦")
    print("=" * 50)
    
    model_path = "/Users/apple/deepseek-qubic-ai/backend/ai/models/deepseek"
    
    try:
        print("ğŸ“Š è¼‰å…¥åˆ†è©å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        print("ğŸ§  è¼‰å…¥æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="cpu"  # å¼·åˆ¶ä½¿ç”¨ CPU
        )
        
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
        # ç°¡å–®æ¸¬è©¦
        test_prompt = "è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼šä»€éº¼æ˜¯å€å¡Šéˆï¼Ÿ"
        print(f"ğŸ“ æ¸¬è©¦æç¤ºè©: {test_prompt}")
        
        # ç·¨ç¢¼
        inputs = tokenizer.encode(test_prompt, return_tensors="pt", max_length=256, truncation=True)
        print(f"ğŸ“ è¼¸å…¥ token æ•¸é‡: {inputs.shape[1]}")
        
        # ç”Ÿæˆ
        print("ğŸš€ é–‹å§‹ç”Ÿæˆ...")
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=50,  # è¼ƒçŸ­çš„ç”Ÿæˆé•·åº¦
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generation_time = time.time() - start_time
        print(f"â±ï¸ ç”Ÿæˆè€—æ™‚: {generation_time:.2f}ç§’")
        
        # è§£ç¢¼
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = full_response[len(test_prompt):].strip()
        
        print(f"âœ… ç”ŸæˆæˆåŠŸ")
        print(f"ğŸ“„ å®Œæ•´å›æ‡‰: {full_response}")
        print(f"ğŸ“ æ–°ç”Ÿæˆå…§å®¹: {response}")
        print(f"ğŸ“ å›æ‡‰é•·åº¦: {len(response)} å­—ç¬¦")
        
        if response and len(response) > 10:
            print("âœ… æ¨¡å‹å·¥ä½œæ­£å¸¸")
            return True
        else:
            print("âŒ æ¨¡å‹å›æ‡‰ç•°å¸¸")
            return False
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_basic_model()
    if success:
        print("\nğŸ‰ åŸºæœ¬æ¨¡å‹æ¸¬è©¦é€šé")
    else:
        print("\nâŒ åŸºæœ¬æ¨¡å‹æ¸¬è©¦å¤±æ•—")
