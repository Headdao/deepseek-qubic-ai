# ä¸‰ VM DeepSeek è’¸é¤¾æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆ

## ğŸ¯ å¯è¡Œæ€§è©•ä¼°ï¼šâœ… é«˜åº¦å¯è¡Œ

åŸºæ–¼ DeepSeek-R1-Distill-Llama-1.5B çš„è¼•é‡åŒ–ç‰¹æ€§ï¼Œä½¿ç”¨ä¸‰å€‹ VM éƒ¨ç½²æ˜¯å®Œå…¨å¯è¡Œçš„æ–¹æ¡ˆã€‚

---

## ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

### æ–¹æ¡ˆ A: æ¨¡å‹æ‹†åˆ†å¼ (æ¨è–¦)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    VM-1         â”‚    â”‚    VM-2         â”‚    â”‚    VM-3         â”‚
â”‚  Orchestrator   â”‚â—„â”€â”€â–ºâ”‚  Compute Node   â”‚â—„â”€â”€â–ºâ”‚  Compute Node   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ ä»»å‹™èª¿åº¦       â”‚    â”‚ â€¢ Embedding     â”‚    â”‚ â€¢ Transformer   â”‚
â”‚ â€¢ è² è¼‰å‡è¡¡       â”‚    â”‚ â€¢ Tokenization  â”‚    â”‚ â€¢ Generation    â”‚
â”‚ â€¢ çµæœèšåˆ       â”‚    â”‚ â€¢ Layer 1-8     â”‚    â”‚ â€¢ Layer 9-16    â”‚
â”‚ â€¢ API Gateway   â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ–¹æ¡ˆ B: ä¸¦è¡Œè™•ç†å¼
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    VM-1         â”‚    â”‚    VM-2         â”‚    â”‚    VM-3         â”‚
â”‚   Load Balancer â”‚    â”‚  Model Instance â”‚    â”‚  Model Instance â”‚
â”‚                 â”‚â—„â”€â”€â–ºâ”‚       A         â”‚    â”‚       B         â”‚
â”‚ â€¢ è«‹æ±‚åˆ†ç™¼       â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ å¥åº·æª¢æŸ¥       â”‚    â”‚ â€¢ å®Œæ•´æ¨¡å‹      â”‚    â”‚ â€¢ å®Œæ•´æ¨¡å‹      â”‚
â”‚ â€¢ çµæœèšåˆ       â”‚    â”‚ â€¢ è™•ç† Query A  â”‚    â”‚ â€¢ è™•ç† Query B  â”‚
â”‚ â€¢ å¿«å–ç®¡ç†       â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» VM é…ç½®éœ€æ±‚

### åŸºæœ¬é…ç½® (æ¯å° VM)
```yaml
minimum_specs:
  cpu: "4 cores (2.5GHz+)"
  memory: "8GB RAM"
  storage: "25GB SSD"
  network: "1Gbps"
  os: "Ubuntu 20.04 LTS"

recommended_specs:
  cpu: "6 cores (3.0GHz+)"
  memory: "16GB RAM"
  storage: "50GB SSD"
  network: "10Gbps"
  gpu: "4GB VRAM (optional)"
```

### é›²ç«¯æœå‹™å•†é¸æ“‡
```yaml
# Google Cloud Platform
gcp_options:
  instance_type: "n1-standard-4"
  cost: "$120/month (3 VMs)"
  gpu_addon: "NVIDIA T4" # +$100/month

# AWS EC2
aws_options:
  instance_type: "m5.xlarge"
  cost: "$140/month (3 VMs)"
  gpu_addon: "NVIDIA T4" # +$90/month

# Azure
azure_options:
  instance_type: "Standard_D4s_v3"
  cost: "$130/month (3 VMs)"
  gpu_addon: "NVIDIA T4" # +$95/month
```

---

## ğŸ› ï¸ æŠ€è¡“å¯¦ç¾

### 1. ç’°å¢ƒè¨­ç½®
```bash
# æ¯å° VM çš„åŸºç¤ç’°å¢ƒ
sudo apt update && sudo apt upgrade -y
sudo apt install python3.9 python3-pip docker.io -y

# Python ç’°å¢ƒ
pip3 install torch transformers accelerate
pip3 install flask redis requests
pip3 install onnxruntime # åŠ é€Ÿæ¨ç†
```

### 2. æ¨¡å‹éƒ¨ç½²ç­–ç•¥

#### VM-1: Orchestrator (å”èª¿å™¨)
```python
# orchestrator.py
from flask import Flask, request, jsonify
import redis
import requests
import time

app = Flask(__name__)
redis_client = redis.Redis(host='localhost', port=6379)

class TaskOrchestrator:
    def __init__(self):
        self.compute_nodes = [
            "http://vm-2:5000",  # Node A
            "http://vm-3:5000"   # Node B
        ]
        self.current_node = 0
    
    def distribute_task(self, prompt):
        # è² è¼‰å‡è¡¡: è¼ªè©¢åˆ†ç™¼
        node_url = self.compute_nodes[self.current_node]
        self.current_node = (self.current_node + 1) % len(self.compute_nodes)
        
        try:
            response = requests.post(
                f"{node_url}/inference",
                json={"prompt": prompt},
                timeout=30
            )
            return response.json()
        except Exception as e:
            # æ•…éšœè½‰ç§»
            backup_node = self.compute_nodes[1 - self.current_node]
            response = requests.post(
                f"{backup_node}/inference",
                json={"prompt": prompt},
                timeout=30
            )
            return response.json()

@app.route('/api/inference', methods=['POST'])
def inference():
    data = request.json
    prompt = data.get('prompt', '')
    
    orchestrator = TaskOrchestrator()
    result = orchestrator.distribute_task(prompt)
    
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

#### VM-2 & VM-3: Compute Nodes (è¨ˆç®—ç¯€é»)
```python
# compute_node.py
from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
import logging

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

class DeepSeekInference:
    def __init__(self):
        self.model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,  # åŠç²¾åº¦ç¯€çœè¨˜æ†¶é«”
            device_map="auto",
            trust_remote_code=True
        )
        
    def generate(self, prompt, max_length=512):
        start_time = time.time()
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                max_length=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        inference_time = time.time() - start_time
        
        return {
            "response": response,
            "inference_time": inference_time,
            "model": self.model_name
        }

# å…¨åŸŸæ¨¡å‹å¯¦ä¾‹ (å•Ÿå‹•æ™‚è¼‰å…¥)
deepseek_model = DeepSeekInference()

@app.route('/inference', methods=['POST'])
def inference():
    try:
        data = request.json
        prompt = data.get('prompt', '')
        
        if not prompt:
            return jsonify({"error": "No prompt provided"}), 400
        
        result = deepseek_model.generate(prompt)
        
        app.logger.info(f"Inference completed in {result['inference_time']:.2f}s")
        return jsonify(result)
        
    except Exception as e:
        app.logger.error(f"Inference error: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        "status": "healthy",
        "model_loaded": deepseek_model is not None,
        "memory_usage": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 3. Docker å®¹å™¨åŒ–éƒ¨ç½²
```dockerfile
# Dockerfile
FROM python:3.9-slim

RUN apt-get update && apt-get install -y \
    wget curl git \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "compute_node.py"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  orchestrator:
    build: .
    ports:
      - "5000:5000"
    environment:
      - ROLE=orchestrator
    volumes:
      - ./orchestrator.py:/app/app.py
    
  compute-node-1:
    build: .
    ports:
      - "5001:5000"
    environment:
      - ROLE=compute
      - NODE_ID=1
    volumes:
      - ./compute_node.py:/app/app.py
      - ./models:/app/models  # æ¨¡å‹å¿«å–
    
  compute-node-2:
    build: .
    ports:
      - "5002:5000"
    environment:
      - ROLE=compute
      - NODE_ID=2
    volumes:
      - ./compute_node.py:/app/app.py
      - ./models:/app/models
```

---

## ğŸ“Š æ€§èƒ½è©•ä¼°

### é æœŸæ€§èƒ½æŒ‡æ¨™
```yaml
performance_metrics:
  single_vm_inference: "3-8 seconds"
  distributed_inference: "4-10 seconds"
  concurrent_requests: "5-10 requests"
  memory_usage: "4-6GB per VM"
  cpu_utilization: "60-80%"
  
throughput_estimates:
  queries_per_minute: "15-30"
  daily_capacity: "20,000+ queries"
  peak_concurrent: "10 users"
```

### æ€§èƒ½å„ªåŒ–ç­–ç•¥
```python
# 1. æ¨¡å‹é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.int8,  # INT8 é‡åŒ–
    device_map="auto"
)

# 2. KV Cache å„ªåŒ–
with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids,
        use_cache=True,  # å•Ÿç”¨ KV cache
        max_length=512
    )

# 3. æ‰¹æ¬¡è™•ç†
def batch_inference(prompts, batch_size=4):
    results = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i+batch_size]
        batch_results = process_batch(batch)
        results.extend(batch_results)
    return results
```

---

## ğŸ”’ éƒ¨ç½²æœ€ä½³å¯¦è¸

### 1. å®‰å…¨æ€§é…ç½®
```bash
# é˜²ç«ç‰†è¨­ç½®
sudo ufw enable
sudo ufw allow 22    # SSH
sudo ufw allow 5000  # API port
sudo ufw allow from <vm-ip-range>  # å…§éƒ¨é€šä¿¡

# SSL æ†‘è­‰ (ä½¿ç”¨ Let's Encrypt)
sudo certbot --nginx -d your-domain.com
```

### 2. ç›£æ§èˆ‡æ—¥èªŒ
```python
# ç›£æ§è…³æœ¬
import psutil
import logging
import time

def monitor_system():
    while True:
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        disk_usage = psutil.disk_usage('/').percent
        
        logging.info(f"CPU: {cpu_usage}%, Memory: {memory_usage}%, Disk: {disk_usage}%")
        
        if cpu_usage > 90 or memory_usage > 90:
            logging.warning("High resource usage detected!")
        
        time.sleep(60)
```

### 3. è‡ªå‹•åŒ–éƒ¨ç½²
```bash
#!/bin/bash
# deploy.sh

echo "ğŸš€ éƒ¨ç½² DeepSeek ä¸‰ VM é›†ç¾¤..."

# VM-1: Orchestrator
ssh vm-1 << 'EOF'
cd /app
git pull origin main
docker-compose up -d orchestrator
EOF

# VM-2 & VM-3: Compute Nodes
for vm in vm-2 vm-3; do
    ssh $vm << 'EOF'
cd /app
git pull origin main
docker-compose up -d compute-node
EOF
done

echo "âœ… éƒ¨ç½²å®Œæˆï¼"
```

---

## ğŸ’° æˆæœ¬åˆ†æ

### é›²ç«¯éƒ¨ç½²æˆæœ¬ (æœˆè²»)
```yaml
infrastructure_costs:
  vm_instances: "$120-140 (3 VMs)"
  storage: "$30 (150GB SSD)"
  network: "$20 (é »å¯¬)"
  monitoring: "$15 (Cloud Monitoring)"
  backup: "$10 (è‡ªå‹•å‚™ä»½)"
  total_monthly: "$195-215"

# æœ¬åœ°éƒ¨ç½²æˆæœ¬ (ä¸€æ¬¡æ€§)
on_premise_costs:
  hardware: "$2000-3000 (3å°ä¼ºæœå™¨)"
  setup: "$500 (å®‰è£è¨­ç½®)"
  maintenance: "$100/month"
```

### ROI åˆ†æ
- **Break-even**: ç´„ 12-15 å€‹æœˆ (vs é›²ç«¯)
- **3å¹´ TCO**: æœ¬åœ° $6500 vs é›²ç«¯ $7500
- **æ¨è–¦**: åˆæœŸä½¿ç”¨é›²ç«¯ï¼Œç©©å®šå¾Œè€ƒæ…®æœ¬åœ°

---

## ğŸ¯ éƒ¨ç½²æª¢æŸ¥æ¸…å–®

### Phase 1: æº–å‚™éšæ®µ âœ…
- [ ] é¸æ“‡é›²ç«¯æœå‹™å•†æˆ–æº–å‚™ç¡¬é«”
- [ ] ç”³è«‹ VM å¯¦ä¾‹ (3å°)
- [ ] è¨­ç½®ç¶²è·¯èˆ‡å®‰å…¨ç¾¤çµ„
- [ ] å®‰è£åŸºç¤è»Ÿé«”ç’°å¢ƒ

### Phase 2: æ¨¡å‹éƒ¨ç½² âœ…
- [ ] ä¸‹è¼‰ DeepSeek æ¨¡å‹
- [ ] æ¸¬è©¦å–®æ©Ÿæ¨ç†
- [ ] é…ç½®åˆ†æ•£å¼æ¶æ§‹
- [ ] éƒ¨ç½² Orchestrator

### Phase 3: æ•´åˆæ¸¬è©¦ âœ…
- [ ] API ç«¯é»æ¸¬è©¦
- [ ] è² è¼‰æ¸¬è©¦
- [ ] æ•…éšœè½‰ç§»æ¸¬è©¦
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦

### Phase 4: ç”Ÿç”¢éƒ¨ç½² âœ…
- [ ] ç›£æ§ç³»çµ±ä¸Šç·š
- [ ] æ—¥èªŒæ”¶é›†é…ç½®
- [ ] å‚™ä»½ç­–ç•¥åŸ·è¡Œ
- [ ] æ–‡æª”å®Œå–„

---

## ğŸ“ˆ æ“´å±•è¦åŠƒ

### çŸ­æœŸæ“´å±• (1-3å€‹æœˆ)
- **å¢åŠ ç¯€é»**: æ“´å±•åˆ° 5-6 å€‹ VM
- **æ¨¡å‹å‡ç´š**: ä½¿ç”¨æ›´å¤§çš„è’¸é¤¾æ¨¡å‹
- **å¿«å–å±¤**: å¢åŠ  Redis é›†ç¾¤
- **API ç¶²é—œ**: ä½¿ç”¨ nginx è² è¼‰å‡è¡¡

### ä¸­æœŸæ“´å±• (3-6å€‹æœˆ)
- **Kubernetes**: å®¹å™¨ç·¨æ’ç®¡ç†
- **å¾®æœå‹™**: æ‹†åˆ†æˆæ›´å¤šå°ˆç”¨æœå‹™
- **GPU åŠ é€Ÿ**: æ·»åŠ  GPU å¯¦ä¾‹
- **CI/CD**: è‡ªå‹•åŒ–éƒ¨ç½²ç®¡é“

### é•·æœŸæ“´å±• (6-12å€‹æœˆ)
- **å¤šå€åŸŸ**: è·¨åœ°åŸŸéƒ¨ç½²
- **é‚Šç·£è¨ˆç®—**: CDN + é‚Šç·£ç¯€é»
- **æ··åˆé›²**: æœ¬åœ° + é›²ç«¯æ··åˆ
- **AI å„ªåŒ–**: æ¨¡å‹è’¸é¤¾èˆ‡é‡åŒ–

---

## ğŸ‰ çµè«–

**âœ… é«˜åº¦å¯è¡Œ**: ä¸‰å€‹ VM è·‘ DeepSeek è’¸é¤¾æ¨¡å‹æ˜¯å®Œå…¨å¯è¡Œçš„æ–¹æ¡ˆ

**æ ¸å¿ƒå„ªå‹¢**:
- ğŸš€ **å¿«é€Ÿéƒ¨ç½²**: 2-3 å¤©å³å¯ä¸Šç·š
- ğŸ’° **æˆæœ¬å¯æ§**: æœˆè²» $200 ä»¥å…§
- ğŸ“ˆ **æ˜“æ–¼æ“´å±•**: æ°´å¹³æ“´å±•ç°¡å–®
- ğŸ”’ **ç©©å®šå¯é **: æœ‰æ•…éšœè½‰ç§»æ©Ÿåˆ¶

**å»ºè­°è¡Œå‹•**:
1. **ç«‹å³é–‹å§‹**: ç”³è«‹ 3å° VM (8GB RAM, 4 cores)
2. **åˆ†éšæ®µéƒ¨ç½²**: å…ˆå–®æ©Ÿæ¸¬è©¦ï¼Œå†åˆ†æ•£å¼
3. **ç›£æ§å„ªåŒ–**: å¯†åˆ‡é—œæ³¨æ€§èƒ½æŒ‡æ¨™
4. **æ–‡æª”è¨˜éŒ„**: è©³ç´°è¨˜éŒ„éƒ¨ç½²éç¨‹

é€™å€‹æ–¹æ¡ˆå°‡ç‚ºæ‚¨çš„ Qubic AI Compute Layer æä¾›å …å¯¦çš„æŠ€è¡“åŸºç¤ï¼ğŸ¯
