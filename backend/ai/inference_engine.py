#!/usr/bin/env python3
"""
DeepSeek AI æ¨ç†å¼•æ“
ç‚º QDashboard æä¾›æ™ºèƒ½åˆ†æåŠŸèƒ½
"""

import os
import sys
import time
import json
import torch
from typing import Dict, Any, List, Optional
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import threading
import logging
from .qubic_knowledge import get_qubic_knowledge_base

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DeepSeekInferenceEngine:
    """DeepSeek æ¨ç†å¼•æ“é¡åˆ¥"""
    
    def __init__(self, model_path: str = "/Users/apple/deepseek-qubic-ai/backend/ai/models/deepseek"):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        """
        self.model_path = Path(model_path)
        self.tokenizer = None
        self.model = None
        self.device = "cpu"  # ä½¿ç”¨ CPU æ¨¡å¼
        self.model_loaded = False
        self.load_lock = threading.Lock()
        
        # åˆå§‹åŒ– Qubic çŸ¥è­˜åº«
        self.qubic_kb = get_qubic_knowledge_base()
        
        # æ¨ç†é…ç½® - é‡å° Qubic çŸ¥è­˜å„ªåŒ–
        self.generation_config = {
            "max_new_tokens": 200,    # ä½¿ç”¨ max_new_tokens è€Œé max_length
            "temperature": 0.6,       # é™ä½æº«åº¦æé«˜æº–ç¢ºæ€§
            "do_sample": True,
            "top_p": 0.8,            # æ›´ä¿å®ˆçš„æ¡æ¨£
            "top_k": 40,             # æ¸›å°‘éš¨æ©Ÿæ€§
            "repetition_penalty": 1.2 # å¢åŠ é‡è¤‡æ‡²ç½°
        }
        
        logger.info(f"DeepSeek æ¨ç†å¼•æ“åˆå§‹åŒ–ï¼Œæ¨¡å‹è·¯å¾‘: {self.model_path}")
        logger.info(f"Qubic çŸ¥è­˜åº«å·²è¼‰å…¥")
    
    def _load_model(self) -> bool:
        """
        è¼‰å…¥æ¨¡å‹å’Œ tokenizer
        
        Returns:
            è¼‰å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.model_loaded:
                return True
            
            with self.load_lock:
                if self.model_loaded:  # é›™é‡æª¢æŸ¥
                    return True
                
                logger.info("é–‹å§‹è¼‰å…¥ DeepSeek æ¨¡å‹...")
                start_time = time.time()
                
                # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ
                model_file = self.model_path / "model.safetensors"
                if not model_file.exists():
                    logger.error(f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {model_file}")
                    return False
                
                # è¼‰å…¥ tokenizer - åŠ å…¥éŒ¯èª¤è™•ç†
                logger.info("è¼‰å…¥ tokenizer...")
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        str(self.model_path),
                        trust_remote_code=True,
                        local_files_only=True  # é¿å…ç¶²è·¯å»¶é²
                    )
                    logger.info("âœ… Tokenizer è¼‰å…¥æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ Tokenizer è¼‰å…¥å¤±æ•—: {e}")
                    return False
                
                # è¼‰å…¥æ¨¡å‹ - åŠ å…¥éŒ¯èª¤è™•ç†
                logger.info("è¼‰å…¥æ¨¡å‹...")
                try:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        str(self.model_path),
                        torch_dtype=torch.float16,  # ä½¿ç”¨ float16 æ¸›å°‘è¨˜æ†¶é«”
                        device_map=self.device,
                        low_cpu_mem_usage=True,
                        trust_remote_code=True,
                        local_files_only=True  # é¿å…ç¶²è·¯å»¶é²
                    )
                    logger.info("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                    return False
                
                # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
                self.model.eval()
                
                load_time = time.time() - start_time
                logger.info(f"æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.2f}ç§’")
                
                self.model_loaded = True
                return True
                
        except Exception as e:
            logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def _clean_response_format(self, response: str) -> str:
        """æ¸…ç†å›æ‡‰æ ¼å¼ï¼Œç§»é™¤éåº¦æ ¼å¼åŒ–"""
        # ç§»é™¤éå¤šçš„è¡¨æƒ…ç¬¦è™Ÿï¼ˆä¿ç•™é©é‡ï¼‰
        import re
        
        # ç§»é™¤é€£çºŒçš„è¡¨æƒ…ç¬¦è™Ÿï¼Œåªä¿ç•™ç¬¬ä¸€å€‹
        response = re.sub(r'([ğŸ“ŠğŸ”¹âœ…âš ï¸ğŸ’¡ğŸ¯ğŸ”ğŸ“ˆğŸ“‰ğŸš€â­ğŸŒŸ]+)\s*([ğŸ“ŠğŸ”¹âœ…âš ï¸ğŸ’¡ğŸ¯ğŸ”ğŸ“ˆğŸ“‰ğŸš€â­ğŸŒŸ]+)', r'\1', response)
        
        # ç§»é™¤éåº¦çš„æ˜Ÿè™Ÿæ ¼å¼
        response = re.sub(r'\*\*([^*]+)\*\*', r'\1', response)
        
        # ç°¡åŒ–åˆ†éš”ç·š
        response = re.sub(r'[-=]{3,}', '', response)
        
        # ç§»é™¤éå¤šçš„æ›è¡Œ
        response = re.sub(r'\n{3,}', '\n\n', response)
        
        # ç¢ºä¿å¥å­çµæ§‹è‡ªç„¶
        response = response.strip()
        
        return response
    
    def generate_response(self, prompt: str, max_length: Optional[int] = None, enhance_with_qubic: bool = True, language: str = "zh-tw", **kwargs) -> str:
        """
        ç”Ÿæˆ AI å›æ‡‰ - æ”¯æ´ Qubic çŸ¥è­˜å¢å¼·
        
        Args:
            prompt: è¼¸å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•·åº¦
            enhance_with_qubic: æ˜¯å¦ä½¿ç”¨ Qubic çŸ¥è­˜å¢å¼·
            **kwargs: å…¶ä»–ç”Ÿæˆåƒæ•¸
            
        Returns:
            ç”Ÿæˆçš„å›æ‡‰æ–‡æœ¬
        """
        try:
            # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
            if not self._load_model():
                logger.warning("âš ï¸ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰")
                if language == "en":
                    return self._get_fallback_english_response(prompt)
                else:
                    return self._get_fallback_qubic_response(prompt)
            
            logger.info(f"ğŸ§  ä½¿ç”¨ DeepSeek æ¨¡å‹ç”Ÿæˆå›æ‡‰ (èªè¨€: {language})")
            
            # ä½¿ç”¨ Qubic çŸ¥è­˜å¢å¼·æç¤º
            enhanced_prompt = prompt
            if enhance_with_qubic:
                enhanced_prompt = self.qubic_kb.enhance_query_with_context(prompt, network_data=None, language=language)
                logger.info(f"ä½¿ç”¨ Qubic çŸ¥è­˜åº«å¢å¼·æç¤º (èªè¨€: {language})")
            
            # æº–å‚™ç”Ÿæˆé…ç½®
            config = self.generation_config.copy()
            if max_length:
                config['max_new_tokens'] = max_length
            config.update(kwargs)
            
            # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
            if not self.model_loaded or self.tokenizer is None or self.model is None:
                raise RuntimeError("æ¨¡å‹å°šæœªè¼‰å…¥ï¼Œè«‹å…ˆå‘¼å« load_model() æ–¹æ³•")
            
            # Tokenize è¼¸å…¥
            inputs = self.tokenizer(enhanced_prompt, return_tensors="pt")
            input_length = inputs['input_ids'].shape[1]
            
            # ç”Ÿæˆå›æ‡‰
            logger.info(f"é–‹å§‹ç”Ÿæˆå›æ‡‰ï¼Œè¼¸å…¥é•·åº¦: {input_length}")
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=150,       # é©ä¸­é•·åº¦
                    temperature=0.6,          # DeepSeek-R1 å»ºè­°æº«åº¦
                    do_sample=True,
                    top_p=0.8,               # é©åº¦å¤šæ¨£æ€§
                    top_k=40,                # å¹³è¡¡é¸æ“‡
                    repetition_penalty=1.2,  # é©åº¦é˜²é‡è¤‡
                    pad_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3   # é¿å…é‡è¤‡
                )
            
            # è§£ç¢¼å›æ‡‰
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"å®Œæ•´å›æ‡‰é•·åº¦: {len(full_response)} å­—ç¬¦")
            logger.info(f"å®Œæ•´å›æ‡‰å‰200å­—ç¬¦: {full_response[:200]}")
            logger.info(f"æç¤ºè©é•·åº¦: {len(enhanced_prompt)} å­—ç¬¦")
            
            # è™•ç† DeepSeek-R1 çš„å›æ‡‰æ ¼å¼
            # é¦–å…ˆç§»é™¤è¼¸å…¥æç¤ºè©
            if full_response.startswith(enhanced_prompt):
                response = full_response[len(enhanced_prompt):].strip()
            else:
                response = full_response.strip()
            
            logger.info(f"ç§»é™¤æç¤ºè©å¾Œçš„å›æ‡‰é•·åº¦: {len(response)}")
            logger.info(f"ç§»é™¤æç¤ºè©å¾Œçš„å›æ‡‰å‰100å­—ç¬¦: {response[:100]}")
            
            # è™•ç† DeepSeek-R1 çš„ <think> æ¨™ç±¤
            if '<think>' in response and '</think>' in response:
                # æå– </think> ä¹‹å¾Œçš„å¯¦éš›å›æ‡‰
                think_end = response.find('</think>')
                if think_end != -1:
                    response = response[think_end + 8:].strip()  # 8 = len('</think>')
                    logger.info(f"ç§»é™¤ <think> æ¨™ç±¤å¾Œ: {response[:50]}")
            
            # å¦‚æœæ˜¯ Qubic å¢å¼·æŸ¥è©¢ï¼Œå˜—è©¦æ‰¾åˆ°æ¨™è¨˜
            if enhance_with_qubic and response:
                answer_markers = ["å°ˆæ¥­åˆ†æï¼š", "åˆ†æï¼š", "Analysis:", "å›ç­”ï¼š", "ç­”æ¡ˆï¼š", "Answer:", "å›æ‡‰ï¼š"]
                found_marker = False
                
                for marker in answer_markers:
                    if marker in response:
                        response = response.split(marker)[-1].strip()
                        logger.info(f"âœ… æ‰¾åˆ°æ¨™è¨˜ '{marker}'ï¼Œæå–å›æ‡‰: {response[:50]}")
                        found_marker = True
                        break
                
                if not found_marker:
                    logger.info("æœªæ‰¾åˆ°ç‰¹å®šæ¨™è¨˜ï¼Œä½¿ç”¨å®Œæ•´å›æ‡‰")
            
            # æœ€çµ‚æ¸…ç†
            if not response or len(response.strip()) < 10:
                logger.warning("å›æ‡‰éçŸ­æˆ–ç‚ºç©ºï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰")
                if language == "en":
                    response = self._get_fallback_english_response(enhanced_prompt)
                else:
                    response = self._get_fallback_qubic_response(enhanced_prompt)
            
            generation_time = time.time() - start_time
            logger.info(f"å›æ‡‰ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {generation_time:.2f}ç§’")
            
            # æ¸…ç†å›æ‡‰æ ¼å¼
            response = self._clean_response_format(response)
            
            # èªè¨€ä¸€è‡´æ€§æª¢æŸ¥
            if language == "en":
                # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                has_chinese = any('\u4e00' <= char <= '\u9fff' for char in response)
                if has_chinese:
                    logger.warning("è‹±æ–‡å›æ‡‰åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œä½¿ç”¨å‚™ç”¨è‹±æ–‡å›æ‡‰")
                    response = self._get_fallback_english_response(prompt)
            
            # é©—è­‰å›æ‡‰å“è³ªï¼ˆåƒ…é‡å° Qubic ç›¸é—œæŸ¥è©¢ï¼‰
            if enhance_with_qubic:
                validation = self.qubic_kb.validate_response(response)
                logger.info(f"å›æ‡‰å“è³ªè©•ä¼°: {validation['quality']} (åˆ†æ•¸: {validation['accuracy_score']})")
                
                # åªæœ‰åœ¨æ¥µç«¯æƒ…æ³ä¸‹æ‰ä½¿ç”¨å‚™ç”¨å›æ‡‰
                if validation['accuracy_score'] < 40:
                    logger.warning(f"å›æ‡‰å“è³ªåä½ (åˆ†æ•¸: {validation['accuracy_score']})ï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰")
                    if language == "en":
                        response = self._get_fallback_english_response(prompt)
                    else:
                        response = self._get_fallback_qubic_response(prompt)
                else:
                    logger.info(f"å›æ‡‰å“è³ªè‰¯å¥½ (åˆ†æ•¸: {validation['accuracy_score']})ï¼Œä½¿ç”¨ AI ç”Ÿæˆå›æ‡‰")
            
            return response if response else "ç”Ÿæˆçš„å›æ‡‰ç‚ºç©ºï¼Œè«‹é‡è©¦ã€‚"
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›æ‡‰å¤±æ•—: {e}")
            return f"éŒ¯èª¤: ç„¡æ³•ç”Ÿæˆå›æ‡‰ - {str(e)}"
    
    def _get_fallback_qubic_response(self, query: str) -> str:
        """ç•¶ AI å›æ‡‰å“è³ªä¸ä½³æ™‚çš„å‚™ç”¨ Qubic å›æ‡‰"""
        query_lower = query.lower()
        
        # å˜—è©¦ç²å–ç•¶å‰ç¶²è·¯æ•¸æ“šä¾†æä¾›å³æ™‚åˆ†æ
        try:
            from backend.app.qubic_client import QubicNetworkClient
            client = QubicNetworkClient()
            tick_info = client.get_tick_info()
            health = client.get_network_health()
            
            current_tick = tick_info.get('tick', 0)
            current_duration = tick_info.get('duration', 0) 
            current_epoch = tick_info.get('epoch', 0)
            health_status = health.get('overall', 'æœªçŸ¥')
            
            # æ ¹æ“šå•é¡Œé¡å‹æä¾›å·®ç•°åŒ–å›æ‡‰
            if any(word in query_lower for word in ['ç¶²è·¯ç‹€æ³', 'ç¶²è·¯ç‹€æ…‹', 'network status', 'åˆ†æ', 'analyze']):
                return f"""åŸºæ–¼ç•¶å‰ç¶²è·¯æ•¸æ“šåˆ†æï¼š
                
Tick: {current_tick:,} - ç©©å®šå¢é•·ï¼Œè™•ç†é€±æœŸæ­£å¸¸
Duration: {current_duration} ç§’ - {"æ¥µä½³è¡¨ç¾ï¼Œé›¶å»¶é²" if current_duration == 0 else "æ­£å¸¸ç¯„åœï¼Œé‹è¡Œé †æš¢" if current_duration <= 2 else "è¼•å¾®å»¶é²ï¼Œéœ€æŒçºŒè§€å¯Ÿ"}
Epoch: {current_epoch} - ç•¶å‰éšæ®µé‹è¡Œä¸­
å¥åº·ç‹€æ…‹: {health_status}

ç¸½çµï¼šç¶²è·¯æ•´é«”é‹è¡Œ{"æ¥µç‚ºé †æš¢" if current_duration == 0 else "ç©©å®šæ­£å¸¸" if current_duration <= 2 else "æœ‰è¼•å¾®æ³¢å‹•"}ï¼Œæ‰€æœ‰æ ¸å¿ƒæŒ‡æ¨™éƒ½åœ¨é æœŸç¯„åœå…§ã€‚"""
            
            elif any(word in query_lower for word in ['å¥åº·', 'health', 'è©•ä¼°', 'evaluate']):
                stability_score = "å„ªç§€" if current_duration == 0 else "è‰¯å¥½" if current_duration <= 2 else "æ™®é€š"
                return f"""ç¶²è·¯å¥åº·ç‹€æ³è©•ä¼°å ±å‘Šï¼š

ç³»çµ±ç©©å®šæ€§: {stability_score} - Duration {current_duration}ç§’è¡¨ç¾{"å“è¶Š" if current_duration == 0 else "ç©©å®š" if current_duration <= 2 else "éœ€é—œæ³¨"}
è™•ç†èƒ½åŠ›: ç•¶å‰ Tick {current_tick:,}ï¼ŒæŒçºŒæ­£å¸¸å¢é•·
é¢¨éšªè©•ä¼°: {"ç„¡é¢¨éšª" if current_duration <= 1 else "ä½é¢¨éšª" if current_duration <= 2 else "ä¸­ç­‰é¢¨éšª"}

å»ºè­°ï¼š{"ç¹¼çºŒä¿æŒç•¶å‰ç‹€æ…‹" if current_duration <= 1 else "æŒçºŒç›£æ§ï¼Œæš«ç„¡ç•°å¸¸" if current_duration <= 2 else "åŠ å¼·ç›£æ§ï¼Œè§€å¯Ÿè¶¨å‹¢è®ŠåŒ–"}"""
            
            elif any(word in query_lower for word in ['epoch', 'é€²åº¦', 'progress', 'é æ¸¬', 'predict']):
                # è¨ˆç®— Epoch é€²åº¦ï¼ˆå‡è¨­æ¯å€‹ Epoch ç´„ 1000 å€‹ Tickï¼‰
                epoch_start_tick = current_epoch * 1000
                epoch_progress = ((current_tick - epoch_start_tick) / 1000) * 100
                remaining_ticks = 1000 - (current_tick - epoch_start_tick)
                estimated_minutes = remaining_ticks * (current_duration + 1) / 60
                
                return f"""Epoch {current_epoch} é€²åº¦é æ¸¬åˆ†æï¼š

ç•¶å‰é€²åº¦: {epoch_progress:.1f}%
å‰©é¤˜ Ticks: ç´„ {remaining_ticks:,}
é ä¼°å®Œæˆæ™‚é–“: {estimated_minutes:.1f} åˆ†é˜ï¼ˆåŸºæ–¼ç•¶å‰ {current_duration}ç§’ Durationï¼‰

è¶¨å‹¢åˆ†æï¼š{"é€²åº¦ç©©å®šï¼Œé è¨ˆæŒ‰æ™‚å®Œæˆ" if current_duration <= 1 else "é€²åº¦æ­£å¸¸ï¼Œé è¨ˆå¦‚æœŸå®Œæˆ" if current_duration <= 2 else "é€²åº¦ç•¥æ…¢ï¼Œå¯†åˆ‡è§€å¯Ÿ"}
æ•ˆç‡è©•ä¼°: {"é«˜æ•ˆç‡" if current_duration == 0 else "æ­£å¸¸æ•ˆç‡" if current_duration <= 2 else "æ•ˆç‡åä½"}"""
        
        except Exception as e:
            if any(word in query_lower for word in ['status', 'health', 'ç‹€æ³', 'å¥åº·']):
                return """Qubic ç¶²è·¯å¥åº·ç‹€æ³ä¸»è¦é€šéä»¥ä¸‹æŒ‡æ¨™è©•ä¼°ï¼š

- Tick: ç¶²è·¯è™•ç†é€±æœŸï¼Œæ‡‰ç©©å®šå¢é•·
- Duration: è™•ç†æ™‚é–“ï¼Œä½æ–¼ 3 ç§’ç‚ºä½³  
- Epoch: ç¶²è·¯éšæ®µï¼Œè½‰æ›æ‡‰é †æš¢
- æ´»èºåœ°å€æ•¸: åæ˜ ç¶²è·¯æ¡ç”¨åº¦

ç•¶å‰ç¶²è·¯é‹è¡Œç‹€æ³å¯é€šé Qubic å®˜æ–¹å·¥å…·ç›£æ§ã€‚"""
        
        if any(word in query_lower for word in ['what', 'definition', 'æ˜¯ä»€éº¼', 'ä»€éº¼æ˜¯']):
            return """Qubic æ˜¯ä¸€å€‹å‰µæ–°çš„å»ä¸­å¿ƒåŒ–è¨ˆç®—å’Œé‡‘èå¹³å°ï¼Œæ¡ç”¨åŸºæ–¼æ³•å®šäººæ•¸çš„é›»è…¦ï¼ˆQBCï¼‰ç³»çµ±ã€‚ä¸»è¦ç‰¹è‰²åŒ…æ‹¬ï¼š

1. æœ‰ç”¨å·¥ä½œé‡è­‰æ˜ï¼ˆUPoWï¼‰- çµåˆå®‰å…¨æ€§å’Œå¯¦ç”¨æ€§çš„å…±è­˜æ©Ÿåˆ¶
2. Qubic Units (QUs) - ç”Ÿæ…‹ç³»çµ±çš„åŸç”Ÿä»£å¹£
3. æ™ºèƒ½åˆç´„æ”¯æ´å’Œé‡å­è¨ˆç®—æŠ—æ€§
4. å®Œæ•´çš„é–‹ç™¼å·¥å…·ç”Ÿæ…‹ç³»çµ±

æ›´å¤šè©³ç´°ä¿¡æ¯è«‹åƒè€ƒ: https://docs.qubic.org/"""
        
        else:
            return """Qubic æ˜¯åŸºæ–¼æ³•å®šäººæ•¸é›»è…¦ï¼ˆQBCï¼‰ç³»çµ±çš„å»ä¸­å¿ƒåŒ–å¹³å°ï¼Œä½¿ç”¨æœ‰ç”¨å·¥ä½œé‡è­‰æ˜ï¼ˆUPoWï¼‰å…±è­˜æ©Ÿåˆ¶ã€‚

é—œéµç‰¹è‰²ï¼š
- é‡å­è¨ˆç®—æŠ—æ€§
- æ™ºèƒ½åˆç´„æ”¯æ´  
- é«˜æ•ˆèƒ½è¨ˆç®—
- å®Œæ•´é–‹ç™¼ç”Ÿæ…‹

è©³ç´°ä¿¡æ¯: https://docs.qubic.org/"""
    
    def _get_fallback_english_response(self, query: str) -> str:
        """è‹±æ–‡å‚™ç”¨å›æ‡‰ç³»çµ±"""
        query_lower = query.lower()
        
        # å˜—è©¦ç²å–ç•¶å‰ç¶²è·¯æ•¸æ“š
        try:
            from backend.app.qubic_client import QubicNetworkClient
            client = QubicNetworkClient()
            tick_info = client.get_tick_info()
            health = client.get_network_health()
            
            current_tick = tick_info.get('tick', 0)
            current_duration = tick_info.get('duration', 0) 
            current_epoch = tick_info.get('epoch', 0)
            health_status = health.get('overall', 'Unknown')
            
            if any(word in query_lower for word in ['status', 'health', 'analysis', 'analyze', 'network', 'performance']):
                return f"""ğŸ“Š **Qubic Network Real-time Analysis**

ğŸ”¹ **Current Metrics**:
- Tick: {current_tick:,} (continuously growing)
- Duration: {current_duration} seconds ({"Excellent" if current_duration == 0 else "Normal" if current_duration <= 2 else "Attention needed"})
- Epoch: {current_epoch}
- Overall Health: {health_status}

ğŸ”¹ **Status Assessment**:
{"âœ… Network running smoothly with excellent processing speed" if current_duration == 0 else "âš ï¸ Network under moderate load, monitoring" if current_duration <= 2 else "ğŸ”´ High network load, requires close monitoring"}

ğŸ”¹ **Recommendations**:
- Continue monitoring Duration metric changes
- Watch for Tick growth trends
- Observe Epoch transition stability

ğŸ’¡ Data sourced from real-time Qubic network status."""
        
        except Exception:
            if any(word in query_lower for word in ['status', 'health', 'analysis', 'network']):
                return """Qubic network health is evaluated through key indicators:

- Tick: Network processing cycles, should grow steadily
- Duration: Processing time, optimal under 3 seconds  
- Epoch: Network phases, transitions should be smooth
- Active addresses: Reflects network adoption

Current network status can be monitored through official Qubic tools."""
        
        if any(word in query_lower for word in ['what', 'definition', 'about']):
            return """Qubic is an innovative decentralized computing and financial platform based on Quorum-based Computer (QBC) system. Key features include:

1. Useful Proof of Work (UPoW) - consensus mechanism combining security and utility
2. Qubic Units (QUs) - native ecosystem token
3. Smart contract support and quantum computing resistance
4. Complete development toolchain ecosystem

For more information, visit: https://docs.qubic.org/"""
        
        else:
            return """Qubic is a decentralized platform based on Quorum-based Computer (QBC) system using Useful Proof of Work (UPoW) consensus.

Key Features:
- Quantum computing resistance
- Smart contract support  
- Decentralized computing
- Qubic Units (QUs) token economy

Learn more at: https://docs.qubic.org/"""
    
    def analyze_qubic_data(self, data: Dict[str, Any], language: str = "zh-tw") -> Dict[str, Any]:
        """
        åˆ†æ Qubic ç¶²è·¯æ•¸æ“š - ä½¿ç”¨å„ªåŒ–çš„æ¨ç†å¼•æ“
        
        Args:
            data: Qubic ç¶²è·¯æ•¸æ“š
            language: å›æ‡‰èªè¨€ ("zh-tw" æˆ– "en")
            
        Returns:
            åˆ†æçµæœ
        """
        try:
            # æ§‹å»ºå„ªåŒ–çš„åˆ†ææç¤º
            prompt = self._build_analysis_prompt(data)
            
            # ä½¿ç”¨å„ªåŒ–çš„ generate_response æ–¹æ³•ï¼ŒåŒ…å«æ‰€æœ‰æ”¹é€²
            analysis = self.generate_response(
                prompt, 
                max_length=300,
                enhance_with_qubic=True,
                language=language
            )
            
            # è§£æå’Œçµæ§‹åŒ–çµæœ
            result = self._parse_analysis_result(analysis, data)
            
            # ç¢ºä¿æˆåŠŸæ¨™è¨˜
            result["success"] = True
            
            logger.info(f"Qubic æ•¸æ“šåˆ†æå®Œæˆï¼Œèªè¨€: {language}")
            return result
            
        except Exception as e:
            logger.error(f"æ•¸æ“šåˆ†æå¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "analysis": "ç„¡æ³•å®Œæˆæ•¸æ“šåˆ†æ",
                "insights": [],
                "recommendations": [],
                "timestamp": int(time.time())
            }
    
    def _build_analysis_prompt(self, data: Dict[str, Any]) -> str:
        """
        å»ºç«‹åˆ†ææç¤º - ä½¿ç”¨ DeepSeek-R1 æœ€ä½³å¯¦è¸å’Œ Qubic çŸ¥è­˜å¢å¼·
        
        Args:
            data: Qubic æ•¸æ“š
            
        Returns:
            æ ¼å¼åŒ–çš„åˆ†ææç¤º
        """
        tick = data.get('tick', 0)
        duration = data.get('duration', 0)
        epoch = data.get('epoch', 0)
        health = data.get('health', {})
        price = data.get('price', 0)
        active_addresses = data.get('activeAddresses', 0)
        
        # ä½¿ç”¨ Qubic çŸ¥è­˜åº«ç²å–åˆ†æä¸Šä¸‹æ–‡
        context = self.qubic_kb.get_relevant_context("network analysis", data)
        
        # æ‡‰ç”¨ DeepSeek-R1 æœ€ä½³å¯¦è¸ï¼š<think> æ¨¡å¼ + çµæ§‹åŒ–åˆ†æ
        prompt = f"""<think>
æˆ‘éœ€è¦å°ç•¶å‰ Qubic ç¶²è·¯æ•¸æ“šé€²è¡Œå°ˆæ¥­ç¶œåˆåˆ†æã€‚
ç•¶å‰é—œéµæŒ‡æ¨™ï¼š
- Tick: {tick:,} (ç¶²è·¯è™•ç†é€±æœŸ)
- Duration: {duration} ç§’ (è™•ç†æ™‚é–“)
- Epoch: {epoch} (ç•¶å‰éšæ®µ)
- å¥åº·ç‹€æ³: {health.get('overall', 'æœªçŸ¥')}
- æ´»èºåœ°å€: {active_addresses:,}

åˆ†æé‡é»ï¼š
1. åŸºæ–¼é€™äº›å…·é«”æ•¸å€¼è©•ä¼°ç¶²è·¯æ€§èƒ½
2. å°æ¯” Qubic ç¶²è·¯çš„æ­£å¸¸é‹è¡Œæ¨™æº–
3. è­˜åˆ¥ä»»ä½•ç•°å¸¸æˆ–å„ªåŒ–æ©Ÿæœƒ
4. æä¾›å¯¦ç”¨çš„ç›£æ§å»ºè­°

æˆ‘éœ€è¦æä¾›å°ˆæ¥­ã€é‡å°æ€§çš„åˆ†æï¼Œé¿å…é€šç”¨æ¨¡æ¿ã€‚
</think>

ä½œç‚ºå°ˆæ¥­çš„ Qubic å€å¡Šéˆç¶²è·¯åˆ†æå¸«ï¼ŒåŸºæ–¼ç•¶å‰ç¶²è·¯æ•¸æ“šé€²è¡Œç¶œåˆè©•ä¼°ï¼š

{context}

ç•¶å‰ç¶²è·¯æŒ‡æ¨™ï¼š
- Tick: {tick:,}
- Duration: {duration} ç§’
- Epoch: {epoch}
- å¥åº·ç‹€æ³: {health.get('overall', 'æœªçŸ¥')}
- åƒ¹æ ¼: ${price:.9f}
- æ´»èºåœ°å€: {active_addresses:,}

è«‹æä¾›å°ˆæ¥­çš„ Qubic ç¶²è·¯åˆ†æï¼ŒåŒ…å«ï¼š
1. ç•¶å‰æ€§èƒ½è©•ä¼°ï¼ˆåŸºæ–¼ Duration å’Œ Tick æŒ‡æ¨™ï¼‰
2. ç¶²è·¯å¥åº·ç‹€æ³åˆ†æ
3. é—œéµæ´å¯Ÿå’Œç™¼ç¾
4. å…·é«”ç›£æ§å»ºè­°

å°ˆæ¥­åˆ†æï¼š"""

        return prompt
    
    def _parse_analysis_result(self, analysis: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æåˆ†æçµæœ
        
        Args:
            analysis: AI ç”Ÿæˆçš„åˆ†ææ–‡æœ¬
            original_data: åŸå§‹æ•¸æ“š
            
        Returns:
            çµæ§‹åŒ–çš„åˆ†æçµæœ
        """
        # æå–é—œéµæ´å¯Ÿ
        insights = self._extract_insights(analysis)
        
        # æå–å»ºè­°
        recommendations = self._extract_recommendations(analysis)
        
        # è¨ˆç®—ä¿¡å¿ƒåº¦
        confidence = self._calculate_confidence(original_data)
        
        return {
            "success": True,
            "analysis": analysis,
            "insights": insights,
            "recommendations": recommendations,
            "confidence": confidence,
            "data_quality": "good" if original_data.get('tick', 0) > 0 else "poor",
            "timestamp": int(time.time())
        }
    
    def _extract_insights(self, analysis: str) -> List[str]:
        """
        å¾åˆ†æä¸­æå–é—œéµæ´å¯Ÿ
        
        Args:
            analysis: åˆ†ææ–‡æœ¬
            
        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        insights = []
        
        # ç°¡å–®çš„é—œéµè©æå–é‚è¼¯
        keywords = [
            "ç¶²è·¯æ­£å¸¸", "æ€§èƒ½è‰¯å¥½", "é‹è¡Œç©©å®š", "å¥åº·ç‹€æ³",
            "ç•°å¸¸", "å»¶é²", "å•é¡Œ", "å»ºè­°", "å„ªåŒ–"
        ]
        
        lines = analysis.split('\n')
        for line in lines:
            line = line.strip()
            if line and any(keyword in line for keyword in keywords):
                insights.append(line)
        
        return insights[:3]  # æœ€å¤šè¿”å› 3 å€‹æ´å¯Ÿ
    
    def _extract_recommendations(self, analysis: str) -> List[str]:
        """
        å¾åˆ†æä¸­æå–å»ºè­°
        
        Args:
            analysis: åˆ†ææ–‡æœ¬
            
        Returns:
            å»ºè­°åˆ—è¡¨
        """
        recommendations = []
        
        # å°‹æ‰¾å»ºè­°ç›¸é—œçš„å…§å®¹
        lines = analysis.split('\n')
        in_recommendations = False
        
        for line in lines:
            line = line.strip()
            if 'å»ºè­°' in line or 'è¡Œå‹•' in line or 'recommendations' in line.lower():
                in_recommendations = True
                continue
            
            if in_recommendations and line:
                if line.startswith(('-', 'â€¢', '1.', '2.', '3.')):
                    recommendations.append(line)
                elif not line[0].isdigit() and len(recommendations) > 0:
                    break
        
        return recommendations[:3]  # æœ€å¤šè¿”å› 3 å€‹å»ºè­°
    
    def _calculate_confidence(self, data: Dict[str, Any]) -> float:
        """
        è¨ˆç®—åˆ†æä¿¡å¿ƒåº¦
        
        Args:
            data: æ•¸æ“šå“è³ª
            
        Returns:
            ä¿¡å¿ƒåº¦ (0.0-1.0)
        """
        confidence = 0.5  # åŸºç¤ä¿¡å¿ƒåº¦
        
        # æ ¹æ“šæ•¸æ“šå“è³ªèª¿æ•´
        if data.get('tick', 0) > 0:
            confidence += 0.2
        
        if 'health' in data and data['health'].get('overall') != 'ç•°å¸¸':
            confidence += 0.2
        
        if data.get('duration', 0) > 0:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def get_status(self) -> Dict[str, Any]:
        """
        ç²å–æ¨ç†å¼•æ“ç‹€æ…‹
        
        Returns:
            ç‹€æ…‹è³‡è¨Š
        """
        return {
            "model_loaded": self.model_loaded,
            "model_path": str(self.model_path),
            "device": self.device,
            "ready": self.model_loaded,
            "last_check": time.strftime("%Y-%m-%d %H:%M:%S")
        }

    def _get_emergency_fallback_response(self, language: str = "zh-tw") -> str:
        """
        ç·Šæ€¥å‚™ç”¨å›æ‡‰ï¼ˆç•¶æ¨¡å‹ç„¡æ³•åŠ è¼‰æ™‚ä½¿ç”¨ï¼‰
        """
        if language == "en":
            return """**System Notice:**
The AI analysis model is currently unavailable. Please try again later or contact system administrator.

**Current Status:** Model loading in progress
**Recommended Action:** Wait a few moments and retry your request"""
        else:
            return """**ç³»çµ±é€šçŸ¥ï¼š**
AI åˆ†ææ¨¡å‹ç›®å‰ç„¡æ³•ä½¿ç”¨ï¼Œè«‹ç¨å¾Œé‡è©¦æˆ–è¯ç¹«ç³»çµ±ç®¡ç†å“¡ã€‚

**ç›®å‰ç‹€æ³ï¼š** æ¨¡å‹è¼‰å…¥ä¸­
**å»ºè­°å‹•ä½œï¼š** è«‹ç¨ç­‰ç‰‡åˆ»å¾Œé‡æ–°å˜—è©¦"""

# å…¨åŸŸæ¨ç†å¼•æ“å¯¦ä¾‹
_inference_engine = None

def get_inference_engine() -> DeepSeekInferenceEngine:
    """
    ç²å–å…¨åŸŸæ¨ç†å¼•æ“å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        æ¨ç†å¼•æ“å¯¦ä¾‹
    """
    global _inference_engine
    if _inference_engine is None:
        _inference_engine = DeepSeekInferenceEngine()
    return _inference_engine
